{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75c7e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cceedba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA API ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbd06787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891947be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f9614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c1d2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHAINING TRANSFORMATIONS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "019042cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb6665b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a413b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.unbatch()\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b245bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1853e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8532355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHUFFLING THE DATA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04348372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "962dd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interleaving lines from multiple files #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b944f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the California dataset to multiple CSV files\n",
    "from sklearn.datasets        import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "# Load and split the data\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid         = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# Compute the mean and stddev of the training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std  = scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "748b9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6915844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data  = np.c_[X_test,  y_test]\n",
    "\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header      = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths  = save_to_multiple_csv_files(test_data,  \"test\",  header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22ffa93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, now let's take a peek at the first few lines of one of these CSV files\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "584db71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets\\\\housing\\\\my_train_00.csv',\n",
       " 'datasets\\\\housing\\\\my_train_01.csv',\n",
       " 'datasets\\\\housing\\\\my_train_02.csv',\n",
       " 'datasets\\\\housing\\\\my_train_03.csv',\n",
       " 'datasets\\\\housing\\\\my_train_04.csv',\n",
       " 'datasets\\\\housing\\\\my_train_05.csv',\n",
       " 'datasets\\\\housing\\\\my_train_06.csv',\n",
       " 'datasets\\\\housing\\\\my_train_07.csv',\n",
       " 'datasets\\\\housing\\\\my_train_08.csv',\n",
       " 'datasets\\\\housing\\\\my_train_09.csv',\n",
       " 'datasets\\\\housing\\\\my_train_10.csv',\n",
       " 'datasets\\\\housing\\\\my_train_11.csv',\n",
       " 'datasets\\\\housing\\\\my_train_12.csv',\n",
       " 'datasets\\\\housing\\\\my_train_13.csv',\n",
       " 'datasets\\\\housing\\\\my_train_14.csv',\n",
       " 'datasets\\\\housing\\\\my_train_15.csv',\n",
       " 'datasets\\\\housing\\\\my_train_16.csv',\n",
       " 'datasets\\\\housing\\\\my_train_17.csv',\n",
       " 'datasets\\\\housing\\\\my_train_18.csv',\n",
       " 'datasets\\\\housing\\\\my_train_19.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51fcd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an input pipeline #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9faf330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e323c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 5 files at a time and interleave their lines, skipping the header\n",
    "n_readers = 5\n",
    "dataset   = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                        cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6751a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
      "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
      "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
     ]
    }
   ],
   "source": [
    "# These are the first rows (ignoring the header row) of five interleaved CSV files, chosen randomly.\n",
    "# Looks good! But as you can see, these are just byte strings; we need to parse them and scale the data.\n",
    "\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adaba54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90c2019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_mean and X_std were computed earlier\n",
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line):\n",
    "    # 8 float attributes with default values zero + 1 label\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y # scale the input features and return the tuple (X_scaled, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4d3bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function preprocess at 0x000001A3DC196790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function preprocess at 0x000001A3DC196790> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c898543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now apply the function to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "09205193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything together #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "781ae6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    \n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21c1bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = tf.Tensor(\n",
      "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
      "   1.2525111  -1.3671792 ]\n",
      " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
      "   0.7231292  -1.0023477 ]\n",
      " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
      "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[1.752]\n",
      " [1.313]\n",
      " [1.535]], shape=(3, 1), dtype=float32)\n",
      "\n",
      "X = tf.Tensor(\n",
      "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
      "   0.9807942  -0.67250353]\n",
      " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
      "   1.107282   -0.8674123 ]\n",
      " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
      "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
      "y = tf.Tensor(\n",
      "[[0.919]\n",
      " [1.028]\n",
      " [2.182]], shape=(3, 1), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "480e35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set  = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98b12c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b7d2061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "362/362 [==============================] - 2s 5ms/step - loss: 1.4679 - val_loss: 21.5124\n",
      "Epoch 2/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.8735 - val_loss: 0.6648\n",
      "Epoch 3/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.6317 - val_loss: 0.6196\n",
      "Epoch 4/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5933 - val_loss: 0.5669\n",
      "Epoch 5/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5629 - val_loss: 0.5402\n",
      "Epoch 6/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5693 - val_loss: 0.5209\n",
      "Epoch 7/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5231 - val_loss: 0.6130\n",
      "Epoch 8/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5074 - val_loss: 0.4818\n",
      "Epoch 9/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.4963 - val_loss: 0.4904\n",
      "Epoch 10/10\n",
      "362/362 [==============================] - 1s 3ms/step - loss: 0.5023 - val_loss: 0.4585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a47820c070>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7eae3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4787752032279968"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c2b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "beefa6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE `TFRecord` FORMAT ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2575976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a very simple binary format that just contains a sequence of binary records of varying sizes.\n",
    "# Each record is comprised of a length, a CRC checksum to check that the length was not corrupted,\n",
    "# then the actual data, and finally a CRC checksum for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a682eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"datasets/my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "928028e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"datasets/my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627c534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d61fd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREPROCESSING THE INPUT FEATURES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b14ca99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) We can prepare our data files before training the model (e.g: using pandas, sklearn, numpy)\n",
    "# 2) Alternatively, we can process our data on the fly when loading it with the Data API (e.g.: using `map()`)\n",
    "# 3) Or we can include a `preprocessing layer` directly in our model. Let's look at this option now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c630f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)        \n",
    "        self.stds_  = np.std( data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42bee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = [1, 2]\n",
    "\n",
    "std_layer = Standardization()\n",
    "std_layer.adapt(data_sample)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(std_layer)\n",
    "# [...]\n",
    "# model.compile([...])\n",
    "# model.fit([...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5129b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0559222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCODING CATEGORICAL FEATURES USING `ONE-HOT VECTORS` ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "21e367db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENCODING CATEGORICAL FEATURES USING `EMBEDDINGS` ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a3e45285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF TRANSFORM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e7606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2ec0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISES ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae315a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Why would you want to use the Data API?\n",
    "\n",
    "# The Data API is useful when the training data doesn't fit in RAM\n",
    "# It allows us to build an efficient pipeline for processing\n",
    "# e.g.: while GPU is processing a batch, CPU is preparing the next batch (this can be done using the `prefatch()` method)\n",
    "\n",
    "# Ingesting a large dataset and preprocessing it efficiently can be a complex engi‐\n",
    "# neering challenge. The Data API makes it fairly simple. It offers many features,\n",
    "# including loading data from various sources (such as text or binary files), reading\n",
    "# data in parallel from multiple sources, transforming it, interleaving the records,\n",
    "# shuffling the data, batching it, and prefetching it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097da2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "# -> Read them in parallel from multiple sources, without needing to load the entire dataset in the RAM\n",
    "# -> Interleave the small files (coarse shuffling) and then shuffle at a finer level (using a shuffling buffer)\n",
    "\n",
    "# Splitting a large dataset into multiple files makes it possible to shuffle it at a\n",
    "# coarse level before shuffling it at a finer level using a shuffling buffer. It also\n",
    "# makes it possible to handle huge datasets that do not fit on a single machine. It’s\n",
    "# also simpler to manipulate thousands of small files rather than one huge file; for example,\n",
    "# it’s easier to split the data into multiple subsets. Lastly, if the data is split\n",
    "# across multiple files spread across multiple servers, it is possible to download sev‐\n",
    "# eral files from different servers simultaneously, which improves the bandwidth usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39a168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "# You can use TensorBoard to visualize profiling data: if the GPU is not fully uti‐\n",
    "# lized then your input pipeline is likely to be the bottleneck. You can fix it by mak‐\n",
    "# ing sure it reads and preprocesses the data in multiple threads in parallel, and\n",
    "# ensuring it prefetches a few batches. If this is insufficient to get your GPU to\n",
    "# 100% usage during training, make sure your preprocessing code is optimized.\n",
    "# You can also try saving the dataset into multiple TFRecord files, and if necessary\n",
    "# perform some of the preprocessing ahead of time so that it does not need to be\n",
    "# done on the fly during training (TF Transform can help with this). If necessary,\n",
    "# use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e96daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    "# A TFRecord file is composed of a sequence of arbitrary binary records: you can\n",
    "# store absolutely any binary data you want in each record. However, in practice\n",
    "# most TFRecord files contain sequences of serialized protocol buffers. This makes\n",
    "# it possible to benefit from the advantages of protocol buffers, such as the fact that\n",
    "# they can be read easily across multiple platforms and languages and their defini‐\n",
    "# tion can be updated later in a backward-compatible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeab7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Why would you go through the hassle of converting all your data to the Example\n",
    "# protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "# The Example protobuf format has the advantage that TensorFlow provides some\n",
    "# operations to parse it (the tf.io.parse*example() functions) without you hav‐\n",
    "# ing to define your own format. It is sufficiently flexible to represent instances in\n",
    "# most datasets. However, if it does not cover your use case, you can define your\n",
    "# own protocol buffer, compile it using protoc (setting the --descriptor_set_out\n",
    "# and --include_imports arguments to export the protobuf descriptor), and use\n",
    "# the tf.io.decode_proto() function to parse the serialized protobufs (see the\n",
    "# “Custom protobuf ” section of the notebook for an example). It’s more compli‐\n",
    "# cated, and it requires deploying the descriptor along with the model, but it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15591b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. When using TFRecords, when would you want to activate compression? Why\n",
    "# not do it systematically?\n",
    "\n",
    "# When using TFRecords, you will generally want to activate compression if the\n",
    "# TFRecord files will need to be downloaded by the training script, as compression\n",
    "# will make files smaller and thus reduce download time. But if the files are located\n",
    "# on the same machine as the training script, it’s usually preferable to leave com‐\n",
    "# pression off, to avoid wasting CPU for decompression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Data can be preprocessed directly when writing the data files, or within the\n",
    "# tf.data pipeline, or in preprocessing layers within your model, or using TF Transform.\n",
    "# Can you list a few pros and cons of each option?\n",
    "\n",
    "# • If you preprocess the data when creating the data files, the training script will\n",
    "# run faster, since it will not have to perform preprocessing on the fly. In some\n",
    "# cases, the preprocessed data will also be much smaller than the original data, so\n",
    "# you can save some space and speed up downloads. It may also be helpful to\n",
    "# materialize the preprocessed data, for example to inspect it or archive it. How‐\n",
    "# ever, this approach has a few cons. First, it’s not easy to experiment with vari‐\n",
    "# ous preprocessing logics if you need to generate a preprocessed dataset for\n",
    "# each variant. Second, if you want to perform data augmentation, you have to\n",
    "# materialize many variants of your dataset, which will use a large amount of\n",
    "# disk space and take a lot of time to generate. Lastly, the trained model will\n",
    "# expect preprocessed data, so you will have to add preprocessing code in your\n",
    "# application before it calls the model.\n",
    "\n",
    "# • If the data is preprocessed with the tf.data pipeline, it’s much easier to tweak\n",
    "# the preprocessing logic and apply data augmentation. Also, tf.data makes it\n",
    "# easy to build highly efficient preprocessing pipelines (e.g., with multithreading\n",
    "# and prefetching). However, preprocessing the data this way will slow down\n",
    "# training. Moreover, each training instance will be preprocessed once per epoch\n",
    "# rather than just once if the data was preprocessed when creating the data files.\n",
    "# Lastly, the trained model will still expect preprocessed data.\n",
    "\n",
    "# • If you add preprocessing layers to your model, you will only have to write the\n",
    "# preprocessing code once for both training and inference. If your model needs\n",
    "# to be deployed to many different platforms, you will not need to write the pre‐\n",
    "# processing code multiple times. Plus, you will not run the risk of using the\n",
    "# wrong preprocessing logic for your model, since it will be part of the model.\n",
    "# On the downside, preprocessing the data will slow down training, and each\n",
    "# training instance will be preprocessed once per epoch. Moreover, by default\n",
    "# the preprocessing operations will run on the GPU for the current batch (you\n",
    "# will not benefit from parallel preprocessing on the CPU, and prefetching). For‐\n",
    "# tunately, the upcoming Keras preprocessing layers should be able to lift the\n",
    "# preprocessing operations from the preprocessing layers and run them as part\n",
    "# of the tf.data pipeline, so you will benefit from multithreaded execution on the\n",
    "# CPU and prefetching.\n",
    "\n",
    "# • Lastly, using TF Transform for preprocessing gives you many of the benefits\n",
    "# from the previous options: the preprocessed data is materialized, each instance\n",
    "# is preprocessed just once (speeding up training), and preprocessing layers get\n",
    "# generated automatically so you only need to write the preprocessing code\n",
    "# once. The main drawback is the fact that you need to learn how to use this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e78d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Name a few common techniques you can use to encode categorical features. What about text?\n",
    "\n",
    "# Ordinal encoding (0 - \"bad\", 1 - \"average\", 2 - \"good\")\n",
    "# One-hot encoding if there are few categories\n",
    "# Embeddings if there are many categories (possibly pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de346b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00bfdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set,\n",
    "# a validation set, and a test set; shuffle the training set; and save each\n",
    "# dataset to multiple TFRecord files. Each record should be a serialized Example\n",
    "# protobuf with two features: the serialized image (use tf.io.serialize_tensor() to serialize each image),\n",
    "# and the label. Then use tf.data to create an efficient dataset for each set.\n",
    "# Finally, use a Keras model to train these datasets, including a\n",
    "# preprocessing layer to standardize each input feature. Try to make the input\n",
    "# pipeline as efficient as possible, using TensorBoard to visualize profiling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c037886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af53d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset keras backend and set the random seed\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "064ff299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf Datasets\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set  = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f335cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BytesList = tf.train.BytesList\n",
    "FloatList = tf.train.FloatList\n",
    "Int64List = tf.train.Int64List\n",
    "Feature   = tf.train.Feature\n",
    "Features  = tf.train.Features\n",
    "Example   = tf.train.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a76fdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a `TFRecord` serialized as `Example` protobuf\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label.numpy()])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a288ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\rI\\000\\000\\001\\004\\000\\000\\000\\000\\001\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000$\\210\\177>6\\000\\000\\000\\001\\003\\004\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000f\\314\\260\\206\\220{\\027\\000\\000\\000\\000\\014\\n\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\233\\354\\317\\262k\\234\\241m@\\027M\\202H\\017\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000E\\317\\337\\332\\330\\330\\243\\177yz\\222\\215X\\254B\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\000\\310\\350\\350\\351\\345\\337\\337\\327\\325\\244\\177{\\304\\345\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\267\\341\\330\\337\\344\\353\\343\\340\\336\\340\\335\\337\\365\\255\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\344\\332\\325\\306\\264\\324\\322\\323\\325\\337\\334\\363\\312\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\003\\000\\014\\333\\334\\324\\332\\300\\251\\343\\320\\332\\340\\324\\342\\305\\3214\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000c\\364\\336\\334\\332\\313\\306\\335\\327\\325\\336\\334\\365w\\2478\\000\\000\\000\\000\\000\\000\\000\\000\\000\\004\\000\\0007\\354\\344\\346\\344\\360\\350\\325\\332\\337\\352\\331\\331\\321\\\\\\000\\000\\000\\001\\004\\006\\007\\002\\000\\000\\000\\000\\000\\355\\342\\331\\337\\336\\333\\336\\335\\330\\337\\345\\327\\332\\377M\\000\\000\\003\\000\\000\\000\\000\\000\\000\\000>\\221\\314\\344\\317\\325\\335\\332\\320\\323\\332\\340\\337\\333\\327\\340\\364\\237\\000\\000\\000\\000\\000\\022,Rk\\275\\344\\334\\336\\331\\342\\310\\315\\323\\346\\340\\352\\260\\274\\372\\370\\351\\356\\327\\000\\0009\\273\\320\\340\\335\\340\\320\\314\\326\\320\\321\\310\\237\\365\\301\\316\\337\\377\\377\\335\\352\\335\\323\\334\\350\\366\\000\\003\\312\\344\\340\\335\\323\\323\\326\\315\\315\\315\\334\\360P\\226\\377\\345\\335\\274\\232\\277\\322\\314\\321\\336\\344\\341\\000b\\351\\306\\322\\336\\345\\345\\352\\371\\334\\302\\327\\331\\361AIju\\250\\333\\335\\327\\331\\337\\337\\340\\345\\035K\\314\\324\\314\\301\\315\\323\\341\\330\\271\\305\\316\\306\\325\\360\\303\\343\\365\\357\\337\\332\\324\\321\\336\\334\\335\\346C0\\313\\267\\302\\325\\305\\271\\276\\302\\300\\312\\326\\333\\335\\334\\354\\341\\330\\307\\316\\272\\265\\261\\254\\265\\315\\316s\\000z\\333\\301\\263\\253\\267\\304\\314\\322\\325\\317\\323\\322\\310\\304\\302\\277\\303\\277\\306\\300\\260\\234\\247\\261\\322\\\\\\000\\000J\\275\\324\\277\\257\\254\\257\\265\\271\\274\\275\\274\\301\\306\\314\\321\\322\\322\\323\\274\\274\\302\\300\\330\\252\\000\\002\\000\\000\\000B\\310\\336\\355\\357\\362\\366\\363\\364\\335\\334\\301\\277\\263\\266\\266\\265\\260\\246\\250c:\\000\\000\\000\\000\\000\\000\\000\\000\\000(=,H)#\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 9\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at one serialized TFRecord\n",
    "for image, label in valid_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c218c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function saves a given dataset to a set of TFRecord files.\n",
    "# The examples are written to the files in a round-robin fashion.\n",
    "# To do this, we enumerate all the examples using the dataset.enumerate() method,\n",
    "# and we compute index % n_shards to decide which file to write to.\n",
    "# We use the standard contextlib.ExitStack class to make sure that all writers are properly closed\n",
    "# whether or not an I/O error occurs while writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9244e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "# Save the training, validation and testing sets as TFRecords, using the function `create_example()` written earlier\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"datasets/tfrecords/{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards) for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path)) for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label) # function written earlier\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e206cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sets\n",
    "train_filepaths = write_tfrecords(\"fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"fashion_mnist.valid\", valid_set)\n",
    "test_filepaths  = write_tfrecords(\"fashion_mnist.test\",  test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82ac422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's use the written TFRecords and generate the tf Dataset (using the tf.data API)\n",
    "\n",
    "# Preprocess a single TFRecord\n",
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64,  default_value=-1)\n",
    "    }\n",
    "    \n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image   = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    image   = tf.reshape(image, shape=[28, 28])\n",
    "    label   = example[\"label\"]\n",
    "    return image, label\n",
    "\n",
    "# Generate a tf Dataset, using the `preprocess()` function written earlier \n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None, n_parse_threads=5, batch_size=32, cache=True):\n",
    "    # Create a tf Dataset and generate a pipeline of transformations\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=n_read_threads)\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "        \n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) # function written earlier\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f7b938fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function preprocess at 0x000001958FA6F040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function preprocess at 0x000001958FA6F040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(valid_filepaths)\n",
    "test_set  = mnist_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0aac80b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot some images from the training set\n",
    "for X, y in train_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(y[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04164725",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "00dae70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_  = np.std( data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "# Create a standardization layer\n",
    "standardization = Standardization(input_shape=[28, 28])\n",
    "\n",
    "# Exclude 100 labels from `sample_image`. This is a representative sample from the training set\n",
    "# with the purpose of `adapting` the standardization layer to the training data (compute `means_` and `stds_`)\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images        = np.concatenate(list(sample_image_batches.as_numpy_iterator()), axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "# Define the model's layers\n",
    "model = keras.models.Sequential([\n",
    "    standardization,                             # preprocess layer (input layer)\n",
    "    keras.layers.Flatten(),                      # preprocess layer\n",
    "    keras.layers.Dense(100, activation=\"relu\"),  # hidden layer\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # output layer (with 10 exclusive classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0ff0edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "      1/Unknown - 0s 1ms/step - loss: 3.0546 - accuracy: 0.0312WARNING:tensorflow:From F:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4490 - accuracy: 0.8413 - val_loss: 0.4055 - val_accuracy: 0.8664\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3385 - accuracy: 0.8789 - val_loss: 0.4027 - val_accuracy: 0.8694\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2946 - accuracy: 0.8924 - val_loss: 0.3583 - val_accuracy: 0.8832\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2710 - accuracy: 0.9017 - val_loss: 0.3654 - val_accuracy: 0.8818\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2503 - accuracy: 0.9079 - val_loss: 0.3792 - val_accuracy: 0.8752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x195e79b8310>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "logs = os.path.join(os.curdir, \"logs/tfrecords/\", \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch=10)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53366bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's open TensorBoard and check the profiling tab (it requires `tensorboard-plugin-profile`)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./logs/tfrecords --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9abbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1ef4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed759fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. In this exercise you will download a dataset, split it, create a `tf.data.Dataset` to load it and\n",
    "# preprocess it efficiently, then build and train a binary classification model containing an Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73ba1ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 19s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Iulian/.keras/datasets/aclImdb')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's download the dataset and store it on the disk\n",
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
    "path     = Path(filepath).parent / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5dae8de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb\\\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n",
      "    test\\\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "        neg\\\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos\\\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "    train\\\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "        neg\\\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos\\\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup\\\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n"
     ]
    }
   ],
   "source": [
    "# Directory structure\n",
    "for name, subdirs, files in os.walk(path):\n",
    "    indent = len(Path(name).parts) - len(path.parts)\n",
    "    print(\"    \" * indent + Path(name).parts[-1] + os.sep)\n",
    "    for index, filename in enumerate(sorted(files)):\n",
    "        if index == 3:\n",
    "            print(\"    \" * (indent + 1) + \"...\")\n",
    "            break\n",
    "        print(\"    \" * (indent + 1) + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "101a7929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split all the paths in 4 categories\n",
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos      = review_paths(path / \"train\" / \"pos\") # path/train/pos\n",
    "train_neg      = review_paths(path / \"train\" / \"neg\") # path/train/neg\n",
    "test_valid_pos = review_paths(path / \"test\"  / \"pos\") # path/test/pos\n",
    "test_valid_neg = review_paths(path / \"test\"  / \"neg\") # path/test/neg\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "efed5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test set into a validation set (15,000) and a test set (10,000)\n",
    "np.random.shuffle(test_valid_pos)\n",
    "np.random.shuffle(test_valid_neg)\n",
    "\n",
    "test_pos  = test_valid_pos[:5000]\n",
    "test_neg  = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "372c6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data to create an efficient dataset for each set. Since the dataset fits in memory,\n",
    "# we can just load all the data using pure Python code and use `tf.data.Dataset.from_tensor_slices()`\n",
    "\n",
    "def imdb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels  = []\n",
    "    \n",
    "    # label 0 = negative class / label 1 = positive class\n",
    "    for filepaths, label in ((filepaths_negative, 0), (filepaths_positive, 1)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath, encoding=\"utf8\") as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "            \n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "58909cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"Airport '77 starts as a brand new luxury 747 plane is loaded up with valuable paintings & such belonging to rich businessman Philip Stevens (James Stewart) who is flying them & a bunch of VIP's to his estate in preparation of it being opened to the public as a museum, also on board is Stevens daughter Julie (Kathleen Quinlan) & her son. The luxury jetliner takes off as planned but mid-air the plane is hi-jacked by the co-pilot Chambers (Robert Foxworth) & his two accomplice's Banker (Monte Markham) & Wilson (Michael Pataki) who knock the passengers & crew out with sleeping gas, they plan to steal the valuable cargo & land on a disused plane strip on an isolated island but while making his descent Chambers almost hits an oil rig in the Ocean & loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the Bermuda Triangle. With air in short supply, water leaking in & having flown over 200 miles off course the problems mount for the survivor's as they await help with time fast running out...<br /><br />Also known under the slightly different tile Airport 1977 this second sequel to the smash-hit disaster thriller Airport (1970) was directed by Jerry Jameson & while once again like it's predecessors I can't say Airport '77 is any sort of forgotten classic it is entertaining although not necessarily for the right reasons. Out of the three Airport films I have seen so far I actually liked this one the best, just. It has my favourite plot of the three with a nice mid-air hi-jacking & then the crashing (didn't he see the oil rig?) & sinking of the 747 (maybe the makers were trying to cross the original Airport with another popular disaster flick of the period The Poseidon Adventure (1972)) & submerged is where it stays until the end with a stark dilemma facing those trapped inside, either suffocate when the air runs out or drown as the 747 floods or if any of the doors are opened & it's a decent idea that could have made for a great little disaster flick but bad unsympathetic character's, dull dialogue, lethargic set-pieces & a real lack of danger or suspense or tension means this is a missed opportunity. While the rather sluggish plot keeps one entertained for 108 odd minutes not that much happens after the plane sinks & there's not as much urgency as I thought there should have been. Even when the Navy become involved things don't pick up that much with a few shots of huge ships & helicopters flying about but there's just something lacking here. George Kennedy as the jinxed airline worker Joe Patroni is back but only gets a couple of scenes & barely even says anything preferring to just look worried in the background.<br /><br />The home video & theatrical version of Airport '77 run 108 minutes while the US TV versions add an extra hour of footage including a new opening credits sequence, many more scenes with George Kennedy as Patroni, flashbacks to flesh out character's, longer rescue scenes & the discovery or another couple of dead bodies including the navigator. While I would like to see this extra footage I am not sure I could sit through a near three hour cut of Airport '77. As expected the film has dated badly with horrible fashions & interior design choices, I will say no more other than the toy plane model effects aren't great either. Along with the other two Airport sequels this takes pride of place in the Razzie Award's Hall of Shame although I can think of lots of worse films than this so I reckon that's a little harsh. The action scenes are a little dull unfortunately, the pace is slow & not much excitement or tension is generated which is a shame as I reckon this could have been a pretty good film if made properly.<br /><br />The production values are alright if nothing spectacular. The acting isn't great, two time Oscar winner Jack Lemmon has said since it was a mistake to star in this, one time Oscar winner James Stewart looks old & frail, also one time Oscar winner Lee Grant looks drunk while Sir Christopher Lee is given little to do & there are plenty of other familiar faces to look out for too.<br /><br />Airport '77 is the most disaster orientated of the three Airport films so far & I liked the ideas behind it even if they were a bit silly, the production & bland direction doesn't help though & a film about a sunken plane just shouldn't be this boring or lethargic. Followed by The Concorde ... Airport '79 (1979).\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X, y in imdb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fa227b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f4a85d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes about 25 seconds to load the dataset and go through it 10 times.\n",
    "# But let's pretend the dataset does not fit in memory, just to make things more interesting\n",
    "# Luckily, each review fits on just one line (they use <br /> to indicate line breaks),\n",
    "# so we can read the reviews using a TextLineDataset.\n",
    "# If they didn't we would have to preprocess the input files (e.g., converting them to TFRecords).\n",
    "# For very large datasets, it would make sense to use a tool like Apache Beam for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d8f42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    # Negative reviews\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative, num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    # Positive reviews\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive, num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    # Return the Dataset\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f51e3b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5fba060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it takes about 54 seconds to go through the dataset 10 times.\n",
    "# That's much slower, essentially because the dataset is not cached in RAM, \n",
    "# so it must be reloaded at each epoch. If you add .cache() just before .repeat(10),\n",
    "# you will see that this implementation will be about as fast as the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9122d682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# This implementation is about as fast as the first one, thanks to the `cache()` method\n",
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acb3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an efficient dataset created using `tf.data`\n",
    "batch_size = 32\n",
    "\n",
    "train_set  = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set  = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set   = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e46c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bd736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Create a binary classification model, using a TextVectorization layer to preprocess each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "053dc866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\n",
       "array([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n",
       "        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n",
       "       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n",
       "        b'<pad>']], dtype=object)>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first write a function to preprocess the reviews, cropping them to 300 characters,\n",
    "# converting them to lower case, then replacing <br /> and all non-letter characters to spaces,\n",
    "# splitting the reviews into words, and finally padding or cropping each review so it ends up with exactly `n_words` tokens:\n",
    "\n",
    "def preprocess(X_batch, n_words=50):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "\n",
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "preprocess(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4e5ffc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'it',\n",
       " b'great',\n",
       " b's',\n",
       " b'a',\n",
       " b'movie',\n",
       " b'i',\n",
       " b'loved',\n",
       " b'was',\n",
       " b'terrible',\n",
       " b'run',\n",
       " b'away']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's write a second utility function that will take a data sample\n",
    "# with the same format as the output of the `preprocess()` function,\n",
    "# and will output the list of the top `max_size` most frequent words, ensuring that the padding token is first\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
    "\n",
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "73f92ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to create the `TextVectorization` layer.\n",
    "# Its `constructor` just saves the hyperparameters (`max_vocabulary_size` and `n_oov_buckets`).\n",
    "# The `adapt()` method computes the vocabulary using the `get_vocabulary()` function, then it builds a `StaticVocabularyTable`.\n",
    "# The `call()`  method preprocesses the reviews to get a padded list of words for each review,\n",
    "# then it uses the `StaticVocabularyTable` to lookup the index of each word in the vocabulary\n",
    "\n",
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets       = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words      = tf.constant(self.vocab)\n",
    "        word_ids   = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "58ddad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization = TextVectorization()\n",
    "\n",
    "text_vectorization.adapt(X_example)\n",
    "text_vectorization(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "95973537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good! As you can see, each review was cleaned up and tokenized,\n",
    "# then each word was encoded as its index in the vocabulary (all the 0s correspond to the <pad> tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f63b6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create another `TextVectorization` layer and let's adapt it to the full IMDB training set (if the training set\n",
    "# did not fit in RAM, we could just use a smaller sample of the training set by calling train_set.take(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e84d5d56",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [3] vs. [2] [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-2c36929861f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtext_vectorization\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mTextVectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_vocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_oov_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtext_vectorization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-150-5b0241916958>\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data_sample)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_vocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mwords\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mword_ids\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-149-9a9cb46b17fc>\u001b[0m in \u001b[0;36mget_vocabulary\u001b[1;34m(data_sample, max_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpreprocessed_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpreprocessed_reviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-148-45e7761ba58a>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(X_batch, n_words)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregex_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"<br\\\\s*/?>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1126\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    507\u001b[0m   \"\"\"\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6166\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6167\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6168\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6842\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6843\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6844\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\envs\\tf-keras-gpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [3] vs. [2] [Op:Mul]"
     ]
    }
   ],
   "source": [
    "max_vocabulary_size   = 1000\n",
    "n_oov_buckets         = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews        = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis=0)\n",
    "\n",
    "text_vectorization    = TextVectorization(max_vocabulary_size, n_oov_buckets, input_shape=[])\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ec0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301f7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c6f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
