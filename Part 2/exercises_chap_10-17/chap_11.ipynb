{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap-11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz8qTXJX_Zrs",
        "outputId": "87ad432f-4000-469a-d2f8-b109180f5be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# TensorFlow ≥2.4 is required in this notebook\n",
        "# Earlier 2.x versions will mostly work the same, but with a few bugs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.4\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"deep\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Practice training a deep neural network on the CIFAR10 image dataset"
      ],
      "metadata": {
        "id": "LPfl3RjwBU05"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the dataset"
      ],
      "metadata": {
        "id": "gmwndIw9W5mb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "1d9rAkPiBU3M"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]"
      ],
      "metadata": {
        "id": "lkwck1GkWlEb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53MUHmlNV94e",
        "outputId": "2cebd8c2-9be1-4892-9470-306b931799f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45000, 32, 32, 3), (45000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.shape, y_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MrRHhMiWwB5",
        "outputId": "370fd59b-410c-47d0-c789-dc76d294b895"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 32, 32, 3), (5000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid.shape, y_valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxlm4_WKWoi7",
        "outputId": "a8bcbbe1-984e-4392-fdac-b1cba1d491fb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 32, 32, 3), (5000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpmjZsBiV96x",
        "outputId": "883086ef-0c8a-4303-c3d3-050742244eeb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "BTBKClMOV988"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a. Build a DNN with 20 hidden layers of 100 neurons each. Use He initialization and the ELU activation function"
      ],
      "metadata": {
        "id": "FRZNxtqZBU5M"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "# Hidden\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100,\n",
        "                               activation='elu',\n",
        "                               kernel_initializer='he_normal'))\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10,\n",
        "                             activation='softmax'))"
      ],
      "metadata": {
        "id": "QUln8SMRBU6_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Nadam(learning_rate=5e-5),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PLM5yYjMW8LN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also add some callbacks we need to train the model"
      ],
      "metadata": {
        "id": "GAXlaurDYSA0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stopping_cb   = keras.callbacks.EarlyStopping(patience=20)\n",
        "\n",
        "# Checkpoint\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_model.h5\", save_best_only=True)\n",
        "\n",
        "# TensorBoard\n",
        "run_index      = 1 # increment every time you train the model\n",
        "run_logdir     = os.path.join(os.curdir, \"cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
      ],
      "metadata": {
        "id": "mHaQ2FPwW8Ne"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train, y_train, epochs=100,\n",
        "#           validation_data=(X_valid, y_valid),\n",
        "#           callbacks=[early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
      ],
      "metadata": {
        "id": "tdaWbnTPYjJU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"cifar10_model.h5\")\n",
        "# model.evaluate(X_valid, y_valid)"
      ],
      "metadata": {
        "id": "gPMzEgrfYsly"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ruH5-H6ZHFg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c. Now try adding Batch Normalization (BN) and compare the learning curves.\n",
        "# Is it converging faster than before? Does it produce a better model?\n",
        "# How does it affect training speed?"
      ],
      "metadata": {
        "id": "tuCtgfc2ZHHv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ZEiQ39G9Zw0k"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "# Hidden\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.Activation('elu'))\n",
        "\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "_ciXFziQZMsC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Nadam(learning_rate=5e-4), # vs. 5e-5 before\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YUdzh6ehZMuT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stopping_cb   = keras.callbacks.EarlyStopping(patience=20)\n",
        "\n",
        "# Checkpoint\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_bn_model.h5\", save_best_only=True)\n",
        "\n",
        "# TensorBoard\n",
        "run_index      = 1 # increment every time you train the model\n",
        "run_logdir     = os.path.join(os.curdir, \"cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
      ],
      "metadata": {
        "id": "Ba69Cy8FZMwN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train, y_train, epochs=100,\n",
        "#           validation_data=(X_valid, y_valid),\n",
        "#           callbacks=[early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
      ],
      "metadata": {
        "id": "6otdMAD1ZMya"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"cifar10_bn_model.h5\")\n",
        "# model.evaluate(X_valid, y_valid)"
      ],
      "metadata": {
        "id": "LrRiHPv2ZM0m"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -> Is the model converging faster than before?\n",
        "# Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs\n",
        "# and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
        "\n",
        "# -> Does BN produce a better model?\n",
        "# Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model,\n",
        "# but at least it's much better than before (a CNN would do much better, but that's a different topic).\n",
        "\n",
        "# -> How does BN affect training speed?\n",
        "# Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers.\n",
        "# But overall the training time (wall time) was shortened significantly!"
      ],
      "metadata": {
        "id": "iGwfZJsBZM2Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xk7a-kxaZM4R"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d. Try replacing Batch Normalization with SELU, and make the necessary adjust ements to ensure the network self-normalizes\n",
        "# (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)"
      ],
      "metadata": {
        "id": "IG5mq8o9ZM6i"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "z2-Eco7jap71"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "# Hidden\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100,\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'))\n",
        "\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "rkbyj4wuap-C"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's standardize the input features\n",
        "# Another option is to add a BN layer after the Flatter layer (as input) => this will normalize the data automatically\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds  = X_train.std(axis=0)\n",
        "\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled  = (X_test - X_means)  / X_stds"
      ],
      "metadata": {
        "id": "VZjZpT0sawte"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Nadam(learning_rate=7e-4), # vs. 5e-5, 5e-4 before\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fZ7-rUe2awv1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stopping_cb   = keras.callbacks.EarlyStopping(patience=20)\n",
        "\n",
        "# Checkpoint\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_self_norm_model.h5\", save_best_only=True)\n",
        "\n",
        "# TensorBoard\n",
        "run_index      = 1 # increment every time you train the model\n",
        "run_logdir     = os.path.join(os.curdir, \"cifar10_logs\", \"run_self_norm_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
      ],
      "metadata": {
        "id": "W4GuxE4Eawxs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train, y_train, epochs=100,\n",
        "#           validation_data=(X_valid, y_valid),\n",
        "#           callbacks=[early_stopping_cb, model_checkpoint_cb, tensorboard_cb])"
      ],
      "metadata": {
        "id": "6cYbVc8_aqAJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"cifar10_self_norm_model.h5\")\n",
        "# model.evaluate(X_valid, y_valid)"
      ],
      "metadata": {
        "id": "0K2ympypZM8s"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yQzVXwnJZHLe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e. Try regularizing the model with alpha dropout. Then, without retraining your\n",
        "# model, see if you can achieve better accuracy using MC Dropout."
      ],
      "metadata": {
        "id": "YbXHzDg7be8w"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "NPuoXkSDbpm_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "# Hiddens (Dense + AlphaDropout)\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100,\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'))\n",
        "  model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "TOO6Oseqbe_I"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I am using AlphaDropout instead of vanilla Dropout because it preserves the self-normalization feature"
      ],
      "metadata": {
        "id": "hm5fK87ibmcv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Nadam(learning_rate=5e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jWI9zgBFbme2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stopping_cb   = keras.callbacks.EarlyStopping(patience=20)\n",
        "\n",
        "# Checkpoint\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
        "\n",
        "# TensorBoard\n",
        "run_index      = 1 # increment every time you train the model\n",
        "run_logdir     = os.path.join(os.curdir, \"cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
      ],
      "metadata": {
        "id": "2Ep0DIB6bmg3"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's fit the model again on the scaled data\n",
        "\n",
        "# model.fit(X_train_scaled, y_train, epochs=100,\n",
        "#           validation_data=(X_valid_scaled, y_valid),\n",
        "#           callbacks=callbacks)"
      ],
      "metadata": {
        "id": "ta7P5U5Xbmit"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"cifar10_alpha_dropout_model.h5\")\n",
        "# model.evaluate(X_valid_scaled, y_valid)"
      ],
      "metadata": {
        "id": "uiuX4t_scT2m"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X8mOUgH1ceTg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use MCDroput now\n",
        "\n",
        "# I created a new class for more robustness (e.g.: if we have some layers which works different at training & inference time BN let's say,\n",
        "# then we can't just 'hardcode' the arg `training=True` on the entire model, because it will break the BN layer)\n",
        "# For more details, see: https://keras.io/api/layers/normalization_layers/batch_normalization/\n",
        "\n",
        "# In conclusion, we will only force the AlphaDropout layer to be active (even at) inference for performing MonteCarlo experiment\n",
        "\n",
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)"
      ],
      "metadata": {
        "id": "IX5YcI2aceVo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers\n",
        "mc_model = keras.models.Sequential([MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model.layers])"
      ],
      "metadata": {
        "id": "SBS9wJcIcT4Y"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities.\n",
        "# The second will use these mean probabilities to predict the most likely class for each instance"
      ],
      "metadata": {
        "id": "s9jJTymsdlDV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
        "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
        "    return np.mean(Y_probas, axis=0)\n",
        "\n",
        "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
        "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
        "    return np.argmax(Y_probas, axis=1)"
      ],
      "metadata": {
        "id": "-qoCSlRfdlFh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's make predictions for all the instances in the validation set, and compute the accuracy"
      ],
      "metadata": {
        "id": "eyQGZvt7bfBM"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "JX-EYWYBd3_D"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
        "# accuracy = np.mean(y_pred == y_valid[:, 0])\n",
        "# accuracy"
      ],
      "metadata": {
        "id": "ESeHvf9Zd1qY"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eQH0ykyRd2u_"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy"
      ],
      "metadata": {
        "id": "eJqjW5fKd2xL"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "2fsvSDikeJ5B"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "# Hiddens (Dense + AlphaDropout)\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100,\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'))\n",
        "  model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2fWUI7N1d2zG"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = keras.backend\n",
        "\n",
        "class ExponentialLearningRate(keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
        "        self.losses.append(logs[\"loss\"])\n",
        "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
        "\n",
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
        "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr])\n",
        "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses\n",
        "\n",
        "def plot_lr_vs_loss(rates, losses):\n",
        "    plt.plot(rates, losses)\n",
        "    plt.gca().set_xscale('log')\n",
        "    plt.hlines(min(losses), min(rates), max(rates))\n",
        "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "u8rlDb6QeXEQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
        "plot_lr_vs_loss(rates, losses)\n",
        "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "0Twa1w3Nd21J",
        "outputId": "8aa34175-81f6-4f06-98f4-91153b9cb335"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "352/352 [==============================] - 10s 23ms/step - loss: nan - accuracy: 0.1042\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9.999999747378752e-06,\n",
              " 9.615227699279785,\n",
              " 2.865111827850342,\n",
              " 4.178337029048375)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1cycle scheduling\n",
        "\n",
        "# Contrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie Smith)\n",
        "# starts by increasing the initial learning rate η0 , growing linearly up to η1\n",
        "# halfway through training. Then it decreases the learning rate linearly down to η0\n",
        "# again during the second half of training, finishing the last few epochs by dropping the rate down\n",
        "# by several orders of magnitude (still linearly). The maximum learning rate η1\n",
        "# is chosen using the same approach we used to find the optimal learning rate (see plot above),\n",
        "# and the initial learning rate η0 is chosen to be roughly 10 times lower.\n",
        "\n",
        "# When using a momentum, we start with a high momentum first\n",
        "# (e.g., 0.95), then drop it down to a lower momentum during the first half of training\n",
        "# (e.g., down to 0.85, linearly), and then bring it back up to the maximum value\n",
        "# (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value."
      ],
      "metadata": {
        "id": "FhTP8Jgfe-kw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see that a good maximum learning rate η1 is 0.05"
      ],
      "metadata": {
        "id": "7HgmGJs0eRhb"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "EwjtJ6hqeRjm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# Input\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "# Hiddens (Dense + AlphaDropout)\n",
        "for _ in range(20):\n",
        "  model.add(keras.layers.Dense(100,\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'))\n",
        "  model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "\n",
        "# Output\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=1e-2),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "pLciq6RmeRl4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OneCycleScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_rate, start_rate=None,\n",
        "                 last_iterations=None, last_rate=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_rate = max_rate\n",
        "        self.start_rate = start_rate or max_rate / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_rate = last_rate or self.start_rate / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
        "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
        "        \n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                     self.max_rate, self.start_rate)\n",
        "        else:\n",
        "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                     self.start_rate, self.last_rate)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.learning_rate, rate)"
      ],
      "metadata": {
        "id": "wSnIz4bcf4gq"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 15\n",
        "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)"
      ],
      "metadata": {
        "id": "tTFdGmKUfhaW"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history  = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
        "#                      validation_data=(X_valid_scaled, y_valid),\n",
        "#                      callbacks=[onecycle])"
      ],
      "metadata": {
        "id": "z0xErxqCfhoy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dQpxnirhgRoO"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}