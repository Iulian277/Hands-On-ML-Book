{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter-16-NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMpfbL/Zu9fvQhEV35m99Ml"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9b33d3b97428433a85be07501370f851":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b565fbf191845369172747c61f6607b","IPY_MODEL_6997dd386e1541978e57c55d4edf8e42","IPY_MODEL_396a27d4be53434bb758f42653855fd3"],"layout":"IPY_MODEL_cf6447e8e75b4f8491665eacd2923ee7"}},"4b565fbf191845369172747c61f6607b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02456c7ae8c04a3585c3e3cf0d12f54f","placeholder":"​","style":"IPY_MODEL_139223a591224db3a2324b2976fb7252","value":"Dl Completed...: 100%"}},"6997dd386e1541978e57c55d4edf8e42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d89c8395824f481096bc6ee68242e435","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5129ba4eeba34addbdd27fe18e8616a1","value":1}},"396a27d4be53434bb758f42653855fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1a9a050c047493e9370b1b38f9dd780","placeholder":"​","style":"IPY_MODEL_12e0d65d23784adebbadc9e144769b30","value":" 1/1 [00:05&lt;00:00,  5.64s/ url]"}},"cf6447e8e75b4f8491665eacd2923ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02456c7ae8c04a3585c3e3cf0d12f54f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"139223a591224db3a2324b2976fb7252":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d89c8395824f481096bc6ee68242e435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5129ba4eeba34addbdd27fe18e8616a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1a9a050c047493e9370b1b38f9dd780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e0d65d23784adebbadc9e144769b30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f144d46ce74a49e88b0117d6c560ed98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3157062ea36b4a1d9717c042bd95e60c","IPY_MODEL_8c8c46dc0ff34aa7b614fce1855f8cca","IPY_MODEL_b3e3128b1b2e4f2a987820a3d01e9869"],"layout":"IPY_MODEL_879735647917436aba2d12388733b260"}},"3157062ea36b4a1d9717c042bd95e60c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e9fdbe02c2c4a21a8a405495e975ea6","placeholder":"​","style":"IPY_MODEL_010d9374999b44e8b8b4e59a2c345cac","value":"Dl Size...: 100%"}},"8c8c46dc0ff34aa7b614fce1855f8cca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5060165e3c34ba5a413eef9350fb7d4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18fb0ae04f43452f878f86338279a92c","value":1}},"b3e3128b1b2e4f2a987820a3d01e9869":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bf02a37550e40d397701a531c1e4c86","placeholder":"​","style":"IPY_MODEL_051a0716ed5f4cf2b739d182ce9f9f9c","value":" 80/80 [00:05&lt;00:00, 35.89 MiB/s]"}},"879735647917436aba2d12388733b260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9fdbe02c2c4a21a8a405495e975ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010d9374999b44e8b8b4e59a2c345cac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5060165e3c34ba5a413eef9350fb7d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"18fb0ae04f43452f878f86338279a92c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0bf02a37550e40d397701a531c1e4c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"051a0716ed5f4cf2b739d182ce9f9f9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"026e7b859753418eb52ed6196df88305":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f669f817c6424c8981c0a82520c8fbf2","IPY_MODEL_bd867c0851c940ffb7391fe72391203f","IPY_MODEL_2abb789c0edf45829bf3105c7c20709f"],"layout":"IPY_MODEL_1d979da1314146dcb89c51c6902694b4"}},"f669f817c6424c8981c0a82520c8fbf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d1e2311765d4a77b933bd0c6f8aed76","placeholder":"​","style":"IPY_MODEL_eafb4b3815614d92958c806432d66674","value":""}},"bd867c0851c940ffb7391fe72391203f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_27a998d512094dd4899a6554e40faea5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65463d120d2d4e7d995dd88069c55e5e","value":1}},"2abb789c0edf45829bf3105c7c20709f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e608704470b74d87b59d24d5a5831fa0","placeholder":"​","style":"IPY_MODEL_cc72f91239824e6ab8431ffe20a72365","value":" 24924/0 [00:11&lt;00:00, 2136.77 examples/s]"}},"1d979da1314146dcb89c51c6902694b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d1e2311765d4a77b933bd0c6f8aed76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eafb4b3815614d92958c806432d66674":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27a998d512094dd4899a6554e40faea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"65463d120d2d4e7d995dd88069c55e5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e608704470b74d87b59d24d5a5831fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc72f91239824e6ab8431ffe20a72365":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a01f3380eac546a99af1e7bbd7bf0feb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5673ae7ffeb3416aba5ce7cbd046fca1","IPY_MODEL_f0edeea69663425abc591084508c7d49","IPY_MODEL_46fd872326cb4bfea293ef7cd794758d"],"layout":"IPY_MODEL_126830f5f44f41b187574332de52d542"}},"5673ae7ffeb3416aba5ce7cbd046fca1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e743afee5b447ea88935b3ad587e0ef","placeholder":"​","style":"IPY_MODEL_0f5168de322e42dbacd53d2631729b2f","value":"100%"}},"f0edeea69663425abc591084508c7d49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ab9467b60824c189017461356a1c626","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70f37ec23efd44989316628c1742b8bd","value":24999}},"46fd872326cb4bfea293ef7cd794758d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41c256e912154493b96c481fdd79add0","placeholder":"​","style":"IPY_MODEL_eece30d6bb784a98b6559ed93eff442b","value":" 24999/25000 [00:00&lt;00:00, 50282.85 examples/s]"}},"126830f5f44f41b187574332de52d542":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e743afee5b447ea88935b3ad587e0ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f5168de322e42dbacd53d2631729b2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ab9467b60824c189017461356a1c626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70f37ec23efd44989316628c1742b8bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41c256e912154493b96c481fdd79add0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eece30d6bb784a98b6559ed93eff442b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9defd9bf5a84031907c25de4190256c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_708eb08366334f16ba5d863e7e5df825","IPY_MODEL_f2ec566ec642441c985d0878e60ceec7","IPY_MODEL_79b30bc3cd214e398504b72012976513"],"layout":"IPY_MODEL_074d4619048a4049bf5c9fc5b3ac8951"}},"708eb08366334f16ba5d863e7e5df825":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26ef0dff8f664010a9ae11dbfd716b87","placeholder":"​","style":"IPY_MODEL_99058aea14214db1bcce344a3bd0cd81","value":""}},"f2ec566ec642441c985d0878e60ceec7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b9edc394d674a9091a44ca4ab5963b0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f7d64a39398427baaed83980b660c28","value":1}},"79b30bc3cd214e398504b72012976513":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd8592ca66494b21befdab088d450dfc","placeholder":"​","style":"IPY_MODEL_842944ea60fe440ebcb44768abf4dd30","value":" 24870/0 [00:08&lt;00:00, 3199.72 examples/s]"}},"074d4619048a4049bf5c9fc5b3ac8951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26ef0dff8f664010a9ae11dbfd716b87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99058aea14214db1bcce344a3bd0cd81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b9edc394d674a9091a44ca4ab5963b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"2f7d64a39398427baaed83980b660c28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd8592ca66494b21befdab088d450dfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"842944ea60fe440ebcb44768abf4dd30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9804b4b3b6d4fa48442a3beb2a3086d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab0c64b4da754a3d90a495fac28c3973","IPY_MODEL_37d4c085a8fc407394888c0ca3298e0c","IPY_MODEL_775bbf9d43f541c092fca3a96b2c1311"],"layout":"IPY_MODEL_95d695a385794b9295f652ea13274202"}},"ab0c64b4da754a3d90a495fac28c3973":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8e977a2036d473eb6df42a3744c21f4","placeholder":"​","style":"IPY_MODEL_fafebd43f3174b26801ba23e0fd4b6b1","value":"100%"}},"37d4c085a8fc407394888c0ca3298e0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_edf87acc3bce48c5952875046568b530","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc8dc35ae0d3497a87fbe9b99f247a50","value":24999}},"775bbf9d43f541c092fca3a96b2c1311":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe92e165db004995855a8c9ab971c468","placeholder":"​","style":"IPY_MODEL_b0d6e6f41d5d45ecbb3712cc8a7c2c8e","value":" 24999/25000 [00:00&lt;00:00, 57519.63 examples/s]"}},"95d695a385794b9295f652ea13274202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8e977a2036d473eb6df42a3744c21f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fafebd43f3174b26801ba23e0fd4b6b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edf87acc3bce48c5952875046568b530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc8dc35ae0d3497a87fbe9b99f247a50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe92e165db004995855a8c9ab971c468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0d6e6f41d5d45ecbb3712cc8a7c2c8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b33d615239f843508d2744322c643163":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c4ce8a879514bd2b0e32f820a59ad80","IPY_MODEL_d953c96ede71437a8a396b98b899ae09","IPY_MODEL_6be43ce3c74e46a48833c1d2d7c96712"],"layout":"IPY_MODEL_c49a0e42c71a427db2cd455e07184f0d"}},"6c4ce8a879514bd2b0e32f820a59ad80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5e468e57dc34dd1a35a8c3e4e233bc4","placeholder":"​","style":"IPY_MODEL_a8d56b455f754528aa0472b94e0ee853","value":""}},"d953c96ede71437a8a396b98b899ae09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_f77182889ddd4915865af80b4d80ba38","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6fb574618f442c3a8fa006bd0d631c9","value":1}},"6be43ce3c74e46a48833c1d2d7c96712":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_644d3344b9914788bbf0e88c2b6a0057","placeholder":"​","style":"IPY_MODEL_e5c5d0f720c341d8bfd29c95133a5018","value":" 49790/0 [00:20&lt;00:00, 3020.84 examples/s]"}},"c49a0e42c71a427db2cd455e07184f0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5e468e57dc34dd1a35a8c3e4e233bc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8d56b455f754528aa0472b94e0ee853":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f77182889ddd4915865af80b4d80ba38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a6fb574618f442c3a8fa006bd0d631c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"644d3344b9914788bbf0e88c2b6a0057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5c5d0f720c341d8bfd29c95133a5018":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01223fe5d3404230a4a3d0a1e4643c2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6da7ff4d71a4ba49932f10e853489c5","IPY_MODEL_072718c2b8274d0baa70eefe89b4727e","IPY_MODEL_e7edc66e31be4b6bad33cbcd4200671e"],"layout":"IPY_MODEL_3251813cd5fb49fdac417d4fc97d4d74"}},"a6da7ff4d71a4ba49932f10e853489c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dc4bf38e4a749b2b8b63d9b2ba19f37","placeholder":"​","style":"IPY_MODEL_d72b41f3e0734a07aa4b407087576c35","value":"100%"}},"072718c2b8274d0baa70eefe89b4727e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_be1a5d80265e4436a7abd968c1c6002a","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d537d7837723466887d3db204edce7b6","value":49999}},"e7edc66e31be4b6bad33cbcd4200671e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_814348536b0d4232a20784fd1ed9a661","placeholder":"​","style":"IPY_MODEL_7f74ca3c73934584bf600e3d317806ae","value":" 49999/50000 [00:00&lt;00:00, 147300.87 examples/s]"}},"3251813cd5fb49fdac417d4fc97d4d74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dc4bf38e4a749b2b8b63d9b2ba19f37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d72b41f3e0734a07aa4b407087576c35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be1a5d80265e4436a7abd968c1c6002a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d537d7837723466887d3db204edce7b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"814348536b0d4232a20784fd1ed9a661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f74ca3c73934584bf600e3d317806ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLSi_pohltkI","executionInfo":{"status":"ok","timestamp":1658475444420,"user_tz":-180,"elapsed":3722,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"9dfeb5be-6fef-4de5-a8e7-360ce0a618d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"]}],"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","if not tf.config.list_physical_devices('GPU'):\n","    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# To make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"nlp\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"]},{"cell_type":"code","source":["### CHAR-RNN ###"],"metadata":{"id":"adzgmF-elxPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Loading and preparing the dataset ##"],"metadata":{"id":"sC3o9vVllxRr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n","with open(filepath) as f:\n","    shakespeare_text = f.read()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6RJoEu8lxT3","executionInfo":{"status":"ok","timestamp":1658387182156,"user_tz":-180,"elapsed":462,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"a945f725-8596-400c-9da3-e272433a69d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n","1130496/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["print(shakespeare_text[:148])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnNEqeD1lxVo","executionInfo":{"status":"ok","timestamp":1658387182158,"user_tz":-180,"elapsed":16,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"836a6a27-0200-4f08-b44b-7fea8d122c71"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n"]}]},{"cell_type":"code","source":["# Let's print all the characters from the text\n","\"\".join(sorted(set(shakespeare_text.lower())))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"6wXs7k1olxYB","executionInfo":{"status":"ok","timestamp":1658387182158,"user_tz":-180,"elapsed":13,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"5fde937a-bd90-4a99-949a-0a44769b9fbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# The default tokenization is at word-level encoding\n","tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) # char_level=True <=> every ch will be treated as a token\n","tokenizer.fit_on_texts(shakespeare_text)"],"metadata":{"id":"P_uvG9jqlxaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.texts_to_sequences([\"First\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FyvzkpXlxc5","executionInfo":{"status":"ok","timestamp":1658387185357,"user_tz":-180,"elapsed":22,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"d23bcb6d-98a7-4b09-859a-5b867828b76f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[20, 6, 9, 8, 3]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n","# Tokenizer lowercases the characters by default"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPdfFlIRl192","executionInfo":{"status":"ok","timestamp":1658387185357,"user_tz":-180,"elapsed":17,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"1698f370-0844-4b0e-cd36-d5f9791b6504"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['f i r s t']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["max_id       = len(tokenizer.word_index) # number of distinct characters\n","dataset_size = tokenizer.document_count  # total number of characters\n","max_id, dataset_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"egVNbVX9l2AH","executionInfo":{"status":"ok","timestamp":1658387185358,"user_tz":-180,"elapsed":9,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"26381fc9-925e-4291-850b-afee4010184c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39, 1115394)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# Let's encode the full text so each ch is represented by its ID\n","# Also, subtract 1 to get IDs from 0 to 38, rather than from 1 to 39\n","[encoded]  = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n","\n","# Take the first 90% of the text for the training set\n","train_size = dataset_size * 90 // 100\n","dataset    = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","# tf.data.Dataset will return each character one by one from this set"],"metadata":{"id":"r5iTYCz4l2CB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The training set now consists of a single sequence of over a million characters\n","n_steps       = 100\n","window_length = n_steps + 1 # target = input shifted 1 character to the right\n","\n","# Shift the window to the right 1 ch for each step\n","# 1st window will have characters 0 to 100\n","# 2nd window will have characters 1 to 101\n","# ...\n","dataset       = dataset.window(window_length, shift=1, drop_remainder=True)\n","\n","# Now, the dataset looks like this:\n","# Dataset{window_1, window_2, ...}, where `window_i` is also a `Dataset` object of length `window_length`"],"metadata":{"id":"f7WpiqXRl2Ez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now, the `dataset` is a neseted dataset. We need to flatten it\n","# Nested dataset -> Flat dataset\n","\n","# We need to convert each `window_i` to a `window_length` dimensions tensor\n","# The dataset will look like this now:\n","# Dataset{batch_1, batch_2, ...}, where `batch_i` is a `Tensor` of `window_length` dimensions\n","\n","dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"metadata":{"id":"RX6o_Fpel2Hc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now, the dataset contains consecutive windows of 101 (`window_length`) characters each"],"metadata":{"id":"BQ2Jr8WUl2JZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","tf.random.set_seed(42)"],"metadata":{"id":"h8MPaBWXl-i5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","# Shuffle the windows from the dataset (not the inner characters)\n","# Then batch the dataset, so that each batch will contain 32 tensors (\"windows\")\n","dataset    = dataset.shuffle(10000).batch(batch_size)\n","\n","# Then, separate the inputs (first 100 chars) from the target (last ch)\n","dataset    = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","\n","# windows[:, :-1] == everything without the last  ch\n","# windows[:, 1:]  == everything without the first ch"],"metadata":{"id":"-QLpyosdl-k_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's encode each character using a one-hot vector, because there are fairly few distinct characters (only 39)\n","dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"],"metadata":{"id":"9yO-GMPLl-nJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, we just need to add prefetching\n","dataset = dataset.prefetch(1)"],"metadata":{"id":"Qf7_JH24l-pA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for X_batch, Y_batch in dataset.take(1):\n","    print(X_batch.shape, Y_batch.shape)\n","# (batch_size=32, (seq_len=100, one_hot_enc_len=39)) (batch_size=32, seq_len_target=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bO7zRAaCl-rC","executionInfo":{"status":"ok","timestamp":1658387192230,"user_tz":-180,"elapsed":5769,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"f9e5e1c5-2969-4def-ab78-089b2d1a807d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 100, 39) (32, 100)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"o8zFeSdul-vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Building and Training the Char-RNN Model ##"],"metadata":{"id":"xkCMweRil-xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# `Warning`: the following code may take up to 24 hours to run, depending on your hardware.\n","# If you use a GPU, it may take just 1 or 2 hours, or less.\n","\n","# `Note`: the GRU class will only use the GPU (if you have one) when using the default values\n","# for the following arguments: [activation, recurrent_activation, recurrent_dropout, unroll, use_bias and reset_after].\n","# This is why I commented out recurrent_dropout=0.2\n","\n","model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")) # max_id = 39 (distinct characters)\n","])\n","\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","\n","# history = model.fit(dataset, epochs=10)"],"metadata":{"id":"2krMashymEMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ..."],"metadata":{"id":"mfTpt2k9mEO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"RS1rJ6jCmERf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Stateful RNN ##"],"metadata":{"id":"V_gnrgN_mEWW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Until now, we have used only `stateless` RNNs: at each training iteration the model\n","# starts with a hidden state full of zeros, then it updates this state at each time step, and\n","# after the last time step, it throws it away, as it is not needed anymore. What if we told\n","# the RNN to preserve this final state after processing one training batch and use it as\n","# the initial state for the next training batch? This way the model can learn long-ter\n","# patterns despite only backpropagating through short sequences. This is called a `stateful` RNN"],"metadata":{"id":"6hcpQB0RmE0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"5kRjIeoUnowl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n","dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n","dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","dataset = dataset.batch(1)\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","dataset = dataset.prefetch(1)\n","# This time, we have batches of size 1 (it's harder to have batches (of 32 length for example), when using stateful RNN)"],"metadata":{"id":"p1YttW9Jnoy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# `Note`: once again, I commented out recurrent_dropout=0.2 so you can get GPU acceleration\n","model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     #dropout=0.2, recurrent_dropout=0.2,\n","                     dropout=0.2,\n","                     batch_input_shape=[batch_size, None, max_id]),\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     #dropout=0.2, recurrent_dropout=0.2),\n","                     dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n","])"],"metadata":{"id":"LyyozGe_no1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# At the end of each *epoch*, we need to reset the states before we go back\n","# to the beginning of the text. For this, we can use a small callback.\n","\n","class ResetStatesCallback(keras.callbacks.Callback):\n","    def on_epoch_begin(self, epoch, logs):\n","        self.model.reset_states()"],"metadata":{"id":"8AKsXnSdno3J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","# history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])"],"metadata":{"id":"It0Q5U48no5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# After this model is trained, it will only be possible to use it to make\n","# predictions for batches of the same size as were used during training.\n","# To avoid this restriction, create an identical stateless model,\n","# and copy the stateful model’s weights to this model."],"metadata":{"id":"8MGhWQMXno7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rTM1iNdRptKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WFvCe3d4ptMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### SENTIMENT ANALYSIS ###"],"metadata":{"id":"3cmdG4yMptOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"_G-YyEpFptQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's load the IMDB dataset\n","(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1B3Z8NIqRAi","executionInfo":{"status":"ok","timestamp":1658387206008,"user_tz":-180,"elapsed":10765,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"19973946-e498-46e5-ad06-6608c4553ce0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["X_train[0][:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRxIizCZqRCo","executionInfo":{"status":"ok","timestamp":1658387206008,"user_tz":-180,"elapsed":9,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"eefc4234-79ec-42c6-fe6a-f5f45db66c80"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# Where are the movie reviews? Well, as you can see, the dataset is already prepro‐\n","# cessed for you: X_train consists of a list of reviews, each of which is represented as a\n","# NumPy array of integers, where each integer represents a word. All punctuation was\n","# removed, and then words were converted to lowercase, split by spaces, and finally\n","# indexed by frequency (so low integers correspond to frequent words). The integers 0,\n","# 1, and 2 are special: they represent the padding token, the start-of-sequence (SSS)\n","# token, and unknown words, respectively. If you want to visualize a review, you can\n","# decode it like this:"],"metadata":{"id":"L_LsoIz6ptSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_index = keras.datasets.imdb.get_word_index()\n","id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n","for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n","    id_to_word[id_] = token\n","\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"kogA1djvqsbK","executionInfo":{"status":"ok","timestamp":1658387206550,"user_tz":-180,"elapsed":546,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"ec7f79ae-0d79-49b2-86f1-3688b60fb636"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1646592/1641221 [==============================] - 0s 0us/step\n","1654784/1641221 [==============================] - 0s 0us/step\n"]},{"output_type":"execute_result","data":{"text/plain":["'<sos> this film was just brilliant casting location scenery story'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# If you want to deploy your model to a mobile device or a web browser, and you don’t\n","# want to have to write a different preprocessing function every time, then you will\n","# want to handle preprocessing using only TensorFlow operations, so it can be included\n","# in the model itself. Let’s see how. First, let’s load the original IMDb reviews, as text\n","# (byte strings), using TensorFlow Datasets"],"metadata":{"id":"HQicIUuQqsfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"],"metadata":{"id":"sibCGwyfr9D6","colab":{"base_uri":"https://localhost:8080/","height":342,"referenced_widgets":["9b33d3b97428433a85be07501370f851","4b565fbf191845369172747c61f6607b","6997dd386e1541978e57c55d4edf8e42","396a27d4be53434bb758f42653855fd3","cf6447e8e75b4f8491665eacd2923ee7","02456c7ae8c04a3585c3e3cf0d12f54f","139223a591224db3a2324b2976fb7252","d89c8395824f481096bc6ee68242e435","5129ba4eeba34addbdd27fe18e8616a1","f1a9a050c047493e9370b1b38f9dd780","12e0d65d23784adebbadc9e144769b30","f144d46ce74a49e88b0117d6c560ed98","3157062ea36b4a1d9717c042bd95e60c","8c8c46dc0ff34aa7b614fce1855f8cca","b3e3128b1b2e4f2a987820a3d01e9869","879735647917436aba2d12388733b260","8e9fdbe02c2c4a21a8a405495e975ea6","010d9374999b44e8b8b4e59a2c345cac","a5060165e3c34ba5a413eef9350fb7d4","18fb0ae04f43452f878f86338279a92c","0bf02a37550e40d397701a531c1e4c86","051a0716ed5f4cf2b739d182ce9f9f9c","026e7b859753418eb52ed6196df88305","f669f817c6424c8981c0a82520c8fbf2","bd867c0851c940ffb7391fe72391203f","2abb789c0edf45829bf3105c7c20709f","1d979da1314146dcb89c51c6902694b4","7d1e2311765d4a77b933bd0c6f8aed76","eafb4b3815614d92958c806432d66674","27a998d512094dd4899a6554e40faea5","65463d120d2d4e7d995dd88069c55e5e","e608704470b74d87b59d24d5a5831fa0","cc72f91239824e6ab8431ffe20a72365","a01f3380eac546a99af1e7bbd7bf0feb","5673ae7ffeb3416aba5ce7cbd046fca1","f0edeea69663425abc591084508c7d49","46fd872326cb4bfea293ef7cd794758d","126830f5f44f41b187574332de52d542","1e743afee5b447ea88935b3ad587e0ef","0f5168de322e42dbacd53d2631729b2f","5ab9467b60824c189017461356a1c626","70f37ec23efd44989316628c1742b8bd","41c256e912154493b96c481fdd79add0","eece30d6bb784a98b6559ed93eff442b","c9defd9bf5a84031907c25de4190256c","708eb08366334f16ba5d863e7e5df825","f2ec566ec642441c985d0878e60ceec7","79b30bc3cd214e398504b72012976513","074d4619048a4049bf5c9fc5b3ac8951","26ef0dff8f664010a9ae11dbfd716b87","99058aea14214db1bcce344a3bd0cd81","4b9edc394d674a9091a44ca4ab5963b0","2f7d64a39398427baaed83980b660c28","cd8592ca66494b21befdab088d450dfc","842944ea60fe440ebcb44768abf4dd30","c9804b4b3b6d4fa48442a3beb2a3086d","ab0c64b4da754a3d90a495fac28c3973","37d4c085a8fc407394888c0ca3298e0c","775bbf9d43f541c092fca3a96b2c1311","95d695a385794b9295f652ea13274202","d8e977a2036d473eb6df42a3744c21f4","fafebd43f3174b26801ba23e0fd4b6b1","edf87acc3bce48c5952875046568b530","fc8dc35ae0d3497a87fbe9b99f247a50","fe92e165db004995855a8c9ab971c468","b0d6e6f41d5d45ecbb3712cc8a7c2c8e","b33d615239f843508d2744322c643163","6c4ce8a879514bd2b0e32f820a59ad80","d953c96ede71437a8a396b98b899ae09","6be43ce3c74e46a48833c1d2d7c96712","c49a0e42c71a427db2cd455e07184f0d","e5e468e57dc34dd1a35a8c3e4e233bc4","a8d56b455f754528aa0472b94e0ee853","f77182889ddd4915865af80b4d80ba38","a6fb574618f442c3a8fa006bd0d631c9","644d3344b9914788bbf0e88c2b6a0057","e5c5d0f720c341d8bfd29c95133a5018","01223fe5d3404230a4a3d0a1e4643c2b","a6da7ff4d71a4ba49932f10e853489c5","072718c2b8274d0baa70eefe89b4727e","e7edc66e31be4b6bad33cbcd4200671e","3251813cd5fb49fdac417d4fc97d4d74","2dc4bf38e4a749b2b8b63d9b2ba19f37","d72b41f3e0734a07aa4b407087576c35","be1a5d80265e4436a7abd968c1c6002a","d537d7837723466887d3db204edce7b6","814348536b0d4232a20784fd1ed9a661","7f74ca3c73934584bf600e3d317806ae"]},"executionInfo":{"status":"ok","timestamp":1658387267193,"user_tz":-180,"elapsed":60648,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"1514cc57-5915-4e41-e236-4d44d739db16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b33d3b97428433a85be07501370f851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f144d46ce74a49e88b0117d6c560ed98"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n"]},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026e7b859753418eb52ed6196df88305"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB77KFF/imdb_reviews-train.tfrecord\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a01f3380eac546a99af1e7bbd7bf0feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9defd9bf5a84031907c25de4190256c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB77KFF/imdb_reviews-test.tfrecord\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9804b4b3b6d4fa48442a3beb2a3086d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33d615239f843508d2744322c643163"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB77KFF/imdb_reviews-unsupervised.tfrecord\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01223fe5d3404230a4a3d0a1e4643c2b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"code","source":["datasets.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xT4pHBAcr9GS","executionInfo":{"status":"ok","timestamp":1658387267193,"user_tz":-180,"elapsed":23,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"de56236d-3238-44ea-cfc5-bcbf9e73ddaf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['test', 'train', 'unsupervised'])"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["train_size = info.splits[\"train\"].num_examples\n","test_size  = info.splits[\"test\"].num_examples"],"metadata":{"id":"Gzl-MdfmqshT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_size, test_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdufX9Fdqsjf","executionInfo":{"status":"ok","timestamp":1658387267194,"user_tz":-180,"elapsed":19,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"c00924b3-275a-4110-fec1-0d54665652c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 25000)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# Let's see some review examples\n","for X_batch, y_batch in datasets[\"train\"].batch(5).take(1):\n","    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n","        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n","        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm3Qf0UEqslM","executionInfo":{"status":"ok","timestamp":1658387267195,"user_tz":-180,"elapsed":17,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"3231d11a-395c-47a1-ad5e-e4cfb62d1c7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n","Label: 0 = Negative\n","\n","Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n","Label: 0 = Negative\n","\n","Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n","Label: 0 = Negative\n","\n","Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful perf ...\n","Label: 1 = Positive\n","\n","Review: As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursu ...\n","Label: 1 = Positive\n","\n"]}]},{"cell_type":"code","source":["def preprocess(X_batch, y_batch):\n","    X_batch = tf.strings.substr(X_batch, 0, 300) # truncate the reviews, keeping only the first 300 characters of each *batch*\n","    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \") # replace <br /> tags      with spaces\n","    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \") # replace any ch != letter with spaces\n","    X_batch = tf.strings.split(X_batch) # split the reviews by spaces => ragged tensor\n","    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch # convert the ragged tensor to dense tensor, padding all reviews with the padding token `<pad>` => same length"],"metadata":{"id":"rrwB63-rsbni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preprocess(X_batch, y_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_erdIKGntglG","executionInfo":{"status":"ok","timestamp":1658387267566,"user_tz":-180,"elapsed":385,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"9ae73fe7-4f33-4d84-f4b9-a8a24757f87c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(5, 59), dtype=string, numpy=\n"," array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n","         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n","         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n","         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n","         b'their', b'worst', b'role', b'in', b'history', b'Even',\n","         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n","         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n","         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n","         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n","         b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n","        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n","         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n","         b'to', b'a', b'combination', b'of', b'things', b'including',\n","         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n","         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n","         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n","         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n","         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n","         b'Cons', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n","         b'<pad>'],\n","        [b'Mann', b'photographs', b'the', b'Alberta', b'Rocky',\n","         b'Mountains', b'in', b'a', b'superb', b'fashion', b'and',\n","         b'Jimmy', b'Stewart', b'and', b'Walter', b'Brennan', b'give',\n","         b'enjoyable', b'performances', b'as', b'they', b'always',\n","         b'seem', b'to', b'do', b'But', b'come', b'on', b'Hollywood',\n","         b'a', b'Mountie', b'telling', b'the', b'people', b'of',\n","         b'Dawson', b'City', b'Yukon', b'to', b'elect', b'themselves',\n","         b'a', b'marshal', b'yes', b'a', b'marshal', b'and', b'to', b'e',\n","         b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n","         b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n","        [b'This', b'is', b'the', b'kind', b'of', b'film', b'for', b'a',\n","         b'snowy', b'Sunday', b'afternoon', b'when', b'the', b'rest',\n","         b'of', b'the', b'world', b'can', b'go', b'ahead', b'with',\n","         b'its', b'own', b'business', b'as', b'you', b'descend', b'into',\n","         b'a', b'big', b'arm', b'chair', b'and', b'mellow', b'for', b'a',\n","         b'couple', b'of', b'hours', b'Wonderful', b'performances',\n","         b'from', b'Cher', b'and', b'Nicolas', b'Cage', b'as', b'always',\n","         b'gently', b'row', b'the', b'plot', b'along', b'There', b'are',\n","         b'no', b'rapids', b'to', b'cr'],\n","        [b'As', b'others', b'have', b'mentioned', b'all', b'the',\n","         b'women', b'that', b'go', b'nude', b'in', b'this', b'film',\n","         b'are', b'mostly', b'absolutely', b'gorgeous', b'The', b'plot',\n","         b'very', b'ably', b'shows', b'the', b'hypocrisy', b'of', b'the',\n","         b'female', b'libido', b'When', b'men', b'are', b'around',\n","         b'they', b'want', b'to', b'be', b'pursued', b'but', b'when',\n","         b'no', b'men', b'are', b'around', b'they', b'become', b'the',\n","         b'pursuers', b'of', b'a', b'year', b'old', b'boy', b'And',\n","         b'the', b'boy', b'becomes', b'<pad>', b'<pad>', b'<pad>']],\n","       dtype=object)>,\n"," <tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 1, 1])>)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# Next, we need to construct the vocabulary. This requires going through the whole\n","# training set once, applying our `preprocess()` function, and using a `Counter` to count\n","# the number of occurrences of each word"],"metadata":{"id":"yeEUsgBst4eB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","\n","vocabulary = Counter()\n","for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n","    for review in X_batch:\n","        vocabulary.update(list(review.numpy()))"],"metadata":{"id":"diuwc2IWsbpy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary.most_common()[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_uJxTcBsbrW","executionInfo":{"status":"ok","timestamp":1658387277916,"user_tz":-180,"elapsed":27,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"1a30a7c2-f769-4344-b20d-03632fefab6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["len(vocabulary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_yCoEwPt7Ts","executionInfo":{"status":"ok","timestamp":1658387277917,"user_tz":-180,"elapsed":24,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"3139c1e5-1c1b-48f3-8a5f-708a6d8a01a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["53893"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# Great! We probably don’t need our model to know all the words in the dictionary to\n","# get good performance, though, so let’s truncate the vocabulary, keeping only the\n","# 10,000 most common words"],"metadata":{"id":"92cMtvbZtyD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = 10000\n","truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"],"metadata":{"id":"QKsxWZfmtyGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n","for word in b\"This movie was faaaaaantastic\".split():\n","    print(word_to_id.get(word) or f\"no ID was found for {word}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Pi7BlAWtyJK","executionInfo":{"status":"ok","timestamp":1658387277918,"user_tz":-180,"elapsed":20,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"33e99e0e-d1e2-4545-c72b-4732e9f6c301"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["22\n","12\n","11\n","no ID was found for b'faaaaaantastic'\n"]}]},{"cell_type":"code","source":["words      = tf.constant(truncated_vocabulary)\n","word_ids   = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n","vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n","\n","num_oov_buckets = 1000\n","table           = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"],"metadata":{"id":"9Mxn-X0BtyLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aF4RPsw0tyOQ","executionInfo":{"status":"ok","timestamp":1658387277918,"user_tz":-180,"elapsed":17,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"110f5e59-8a03-40aa-b2bb-dd59d582ae22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# This is the final preprocessing step for our training dataset\n","def encode_words(X_batch, y_batch):\n","    return table.lookup(X_batch), y_batch\n","\n","train_set = datasets[\"train\"].batch(32).map(preprocess)\n","train_set = train_set.map(encode_words).prefetch(1)"],"metadata":{"id":"lvzaDIMDtyP8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for X_batch, y_batch in train_set.take(1):\n","    print(X_batch)\n","    print(y_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypnOA9Xrsbtd","executionInfo":{"status":"ok","timestamp":1658387278273,"user_tz":-180,"elapsed":5,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"6271b7b7-5630-4437-d4b6-b8f031cc675c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[  22   11   28 ...    0    0    0]\n"," [   6   21   70 ...    0    0    0]\n"," [4099 6881    1 ...    0    0    0]\n"," ...\n"," [  22   12  118 ...  331 1047    0]\n"," [1757 4101  451 ...    0    0    0]\n"," [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n","tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"]}]},{"cell_type":"code","source":["# Now, we can create the model and train it\n","embed_size = 128\n","\n","model = keras.models.Sequential([\n","    keras.layers.Embedding(vocab_size + num_oov_buckets,\n","                           embed_size,\n","                           mask_zero=True, # this means that padding tokens (IDS 0) will be ignored by all downstream layers\n","                           input_shape=[None]),\n","    keras.layers.GRU(128, return_sequences=True),\n","    keras.layers.GRU(128),\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","history = model.fit(train_set, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlMZJmaNsbvS","executionInfo":{"status":"ok","timestamp":1658387971296,"user_tz":-180,"elapsed":693026,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"74e842eb-8bf6-42ab-f2e0-0859ff325aba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","782/782 [==============================] - 143s 172ms/step - loss: 0.5305 - accuracy: 0.7281\n","Epoch 2/5\n","782/782 [==============================] - 132s 169ms/step - loss: 0.3459 - accuracy: 0.8549\n","Epoch 3/5\n","782/782 [==============================] - 135s 173ms/step - loss: 0.1934 - accuracy: 0.9314\n","Epoch 4/5\n","782/782 [==============================] - 132s 169ms/step - loss: 0.1360 - accuracy: 0.9503\n","Epoch 5/5\n","782/782 [==============================] - 132s 169ms/step - loss: 0.1032 - accuracy: 0.9634\n"]}]},{"cell_type":"code","source":["## Reusing Pretrained Embeddings ##"],"metadata":{"id":"VOF8mo0Csbxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(42)"],"metadata":{"id":"W4t99AyWx3pq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n","os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"],"metadata":{"id":"0Y7EKiE8x3rv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow_hub as hub\n","\n","model = keras.Sequential([\n","    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n","                   dtype=tf.string, input_shape=[], output_shape=[50]),\n","    keras.layers.Dense(128, activation=\"relu\"),\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"gugTsx3qx3uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n","    for filename in filenames:\n","        print(os.path.join(dirpath, filename))"],"metadata":{"id":"sHgm7QOSx3vt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658387975396,"user_tz":-180,"elapsed":11,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"a5642aaa-237a-44f6-db5f-709baf5a2a4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n","./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n","./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n","./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n","./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n"]}]},{"cell_type":"code","source":["# Next, we can just load the IMDb reviews dataset — no need to preprocess it\n","# (except for batching and prefetching) — and directly train the model"],"metadata":{"id":"Pe4NlOEeqsoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n","train_size = info.splits[\"train\"].num_examples\n","batch_size = 32\n","train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n","history = model.fit(train_set, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E0fqwnrB1wQB","executionInfo":{"status":"ok","timestamp":1658388008907,"user_tz":-180,"elapsed":33517,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"caf159d4-2701-49bf-d3f3-580027343acc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","782/782 [==============================] - 5s 6ms/step - loss: 0.5461 - accuracy: 0.7267\n","Epoch 2/5\n","782/782 [==============================] - 5s 6ms/step - loss: 0.5130 - accuracy: 0.7493\n","Epoch 3/5\n","782/782 [==============================] - 5s 7ms/step - loss: 0.5082 - accuracy: 0.7522\n","Epoch 4/5\n","782/782 [==============================] - 7s 9ms/step - loss: 0.5047 - accuracy: 0.7544\n","Epoch 5/5\n","782/782 [==============================] - 6s 7ms/step - loss: 0.5018 - accuracy: 0.7557\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"VotwvMZYlEUH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rbsqO4izD5wb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### EXERCISES ###"],"metadata":{"id":"CcHT8I1XD5yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n","\n","# Stateless RNNs can only capture patterns whose length is less than, or equal to,\n","# the size of the windows the RNN is trained on. Conversely, stateful RNNs can\n","# capture longer-term patterns. However, implementing a stateful RNN is much\n","# harder—especially preparing the dataset properly. Moreover, stateful RNNs do\n","# not always work better, in part because consecutive batches are not independent\n","# and identically distributed (IID). Gradient Descent is not fond of non-IID datasets"],"metadata":{"id":"UfIGJoC-D50i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n","\n","# In general, if you translate a sentence one word at a time, the result will be terri‐\n","# ble. For example, the French sentence “Je vous en prie” means “You are welcome,”\n","# but if you translate it one word at a time, you get “I you in pray.” Huh? It is much\n","# better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN\n","# would start translating a sentence immediately after reading the\n","# first word, while an Encoder–Decoder RNN will first read the whole sentence\n","# and then translate it. That said, one could imagine a plain sequence-to-sequence\n","# RNN that would output silence whenever it is unsure about what to say next (just\n","# like human translators do when they must translate a live broadcast)."],"metadata":{"id":"O_6YmN2SyMxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. How can you deal with variable-length input sequences? What about variable length output sequences?\n","\n","# Variable-length input sequences can be handled by padding the shorter sequen‐\n","# ces so that all sequences in a batch have the same length, and using masking to\n","# ensure the RNN ignores the padding token. For better performance, you may\n","# also want to create batches containing sequences of similar sizes. Ragged tensors\n","# can hold sequences of variable lengths, and tf.keras will likely support them even‐\n","# tually, which will greatly simplify handling variable-length input sequences (at\n","# the time of this writing, it is not the case yet). Regarding variable-length output\n","# sequences, if the length of the output sequence is known in advance (e.g., if you\n","# know that it is the same as the input sequence), then you just need to configure\n","# the loss function so that it ignores tokens that come after the end of the sequence.\n","# Similarly, the code that will use the model should ignore tokens beyond the end\n","# of the sequence. But generally the length of the output sequence is not known\n","# ahead of time, so the solution is to train the model so that it outputs an\n","# end-of-sequence token at the end of each sequence."],"metadata":{"id":"Tt7WVWvDyMzw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. What is beam search and why would you use it? What tool can you use to implement it?\n","\n","# Beam search is a technique used to improve the performance of a trained\n","# Encoder–Decoder model, for example in a neural machine translation system.\n","# The algorithm keeps track of a short list of the k most promising output senten‐\n","# ces (say, the top three), and at each decoder step it tries to extend them by one\n","# word; then it keeps only the k most likely sentences. The parameter k is called the\n","# beam width: the larger it is, the more CPU and RAM will be used, but also the\n","# more accurate the system will be. Instead of greedily choosing the most likely\n","# next word at each step to extend a single sentence, this technique allows the sys‐\n","# tem to explore several promising sentences simultaneously. Moreover, this tech‐\n","# nique lends itself well to parallelization. You can implement beam search fairly\n","# easily using TensorFlow Addons."],"metadata":{"id":"MUD0etmhyM1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. What is an attention mechanism? How does it help?\n","\n","# An attention mechanism is a technique initially used in Encoder–Decoder mod‐\n","# els to give the decoder more direct access to the input sequence, allowing it to\n","# deal with longer input sequences. At each decoder time step, the current decod‐\n","# er’s state and the full output of the encoder are processed by an alignment model\n","# that outputs an alignment score for each input time step. This score indicates\n","# which part of the input is most relevant to the current decoder time step. The\n","# weighted sum of the encoder output (weighted by their alignment score) is then\n","# fed to the decoder, which produces the next decoder state and the output for this\n","# time step. The main benefit of using an attention mechanism is the fact that the\n","# Encoder–Decoder model can successfully process longer input sequences.\n","# Another benefit is that the alignment scores makes the model easier to debug and\n","# interpret: for example, if the model makes a mistake, you can look at which part\n","# of the input it was paying attention to, and this can help diagnose the issue. An\n","# attention mechanism is also at the core of the Transformer architecture, in the\n","# Multi-Head Attention layers."],"metadata":{"id":"-y4wo75429Z4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6. What is the most important layer in the Transformer architecture? What is its purpose?\n","\n","# The most important layer in the Transformer architecture is the Multi-Head\n","# Attention layer (the original Transformer architecture contains 18 of them,\n","# including 6 Masked Multi-Head Attention layers). It is at the core of language\n","# models such as BERT and GPT-2. Its purpose is to allow the model to identify\n","# which words are most aligned with each other, and then improve each word’s\n","# representation using these contextual clues"],"metadata":{"id":"hWmfbhQP0SAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 7. When would you need to use sampled softmax?\n","\n","# Sampled softmax is used when training a classification model when there are\n","# many classes (e.g., thousands). It computes an approximation of the crossentropy loss\n","# based on the logit predicted by the model for the correct class, and\n","# the predicted logits for a sample of incorrect words. This speeds up training con‐\n","# siderably compared to computing the softmax over all logits and then estimating\n","# the cross-entropy loss. After training, the model can be used normally, using the\n","# regular softmax function to compute all the class probabilities based on all the\n","# logits."],"metadata":{"id":"LlBqmjak0SCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MB8Q-u2t0SEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jVj8xvwd--RI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KbD4ObNY0SGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 8. Embedded Reber grammars were used by Hochreiter and Schmidhuber in their\n","# paper about LSTMs. They are artificial grammars that produce strings such as\n","# “BPBTSXXVPSEPE.” Check out Jenny Orr’s nice introduction to this topic.\n","# Choose a particular embedded Reber grammar (such as the one represented on\n","# Jenny Orr’s page), then train an RNN to identify whether a string respects that\n","# grammar or not. You will first need to write a function capable of generating a\n","# training batch containing about 50% strings that respect the grammar, and 50% that don’t."],"metadata":{"id":"r3t3sIbd29bw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First we need to build a function that generates strings based on a grammar.\n","# The grammar will be represented as a list of possible transitions for each state.\n","# A transition specifies the string to output (or a grammar to generate it) and the next state."],"metadata":{"id":"f6PN6t0F2nd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This is the chosen Reber grammar: https://www.willamette.edu/~gorr/classes/cs449/reber.html"],"metadata":{"id":"EvHnpwf9_25s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List index = Current state\n","# Grammar is represented as a list of lists,\n","# where each inner list defines all the next conditional states"],"metadata":{"id":"YhIjt3dH2ngO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["default_reber_grammar = [\n","    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n","    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n","    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n","    [(\"T\", 3), (\"V\", 5)], # and so on...\n","    [(\"X\", 3), (\"S\", 6)],\n","    [(\"P\", 4), (\"V\", 6)],\n","    [(\"E\", None)]]        # (state 6) =E=>(terminal state)"],"metadata":{"id":"KHhV4GNC2nie"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedded_reber_grammar = [\n","    [(\"B\", 1)],\n","    [(\"T\", 2), (\"P\", 3)],\n","    [(default_reber_grammar, 4)], # \"recursive\"\n","    [(default_reber_grammar, 5)], # \"recursive\"\n","    [(\"T\", 6)],\n","    [(\"P\", 6)],\n","    [(\"E\", None)]]"],"metadata":{"id":"23DeOGvZ7d4l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_string(grammar):\n","    state = 0\n","    output = []\n","    while state is not None:\n","        index = np.random.randint(len(grammar[state]))\n","        production, state = grammar[state][index]\n","        if isinstance(production, list): # if is `default_reber_grammar` type (\"recursive\" from the above cell)\n","            production = generate_string(grammar=production)\n","        output.append(production)\n","    return \"\".join(output)"],"metadata":{"id":"U4wm5Zg12nkP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's generate a few strings based on the `default` Reber grammar\n","np.random.seed(42)\n","\n","for _ in range(25):\n","    print(generate_string(default_reber_grammar), end=\" \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BDi0F0f22nmH","executionInfo":{"status":"ok","timestamp":1658409905908,"user_tz":-180,"elapsed":244,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"1573940e-86a4-48dc-add1-bb5b5ba20fd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "]}]},{"cell_type":"code","source":["# Looks good. Now let's generate a few strings based on the `embedded` Reber grammar\n","np.random.seed(42)\n","\n","for _ in range(25):\n","    print(generate_string(embedded_reber_grammar), end=\" \")"],"metadata":{"id":"r_banS0emE9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658409927525,"user_tz":-180,"elapsed":269,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"c73c6d14-1e05-4516-d429-994fefae76d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "]}]},{"cell_type":"code","source":["# Okay, now we need a function to generate strings that do not respect the grammar.\n","# We could generate a random string, but the task would be a bit too easy,\n","# so instead we will generate a string that respects the grammar,\n","# and we will corrupt it by changing just one character"],"metadata":{"id":"z0QoBe217rsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["POSSIBLE_CHARS = \"BEPSTVX\"\n","\n","def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n","    good_string = generate_string(grammar) # generate a valid Reber string\n","    index       = np.random.randint(len(good_string))\n","    good_char   = good_string[index]\n","    bad_char    = np.random.choice(sorted(set(chars) - set(good_char)))\n","    return good_string[:index] + bad_char + good_string[index + 1:] # intersperse the `bad_char` in the `good_string`"],"metadata":{"id":"RYu3CuU97rvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's look at a few corrupted strings\n","np.random.seed(42)\n","\n","for _ in range(25):\n","    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anH-Z9oy79eL","executionInfo":{"status":"ok","timestamp":1658411168130,"user_tz":-180,"elapsed":286,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"02d27ec0-7564-42da-8f5a-fc006420c9d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "]}]},{"cell_type":"code","source":["# We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character.\n","# Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters,\n","# one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs.\n","# Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\""],"metadata":{"id":"tUcLKaax79gH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def string_to_ids(s, chars=POSSIBLE_CHARS):\n","    return [chars.index(c) for c in s]"],"metadata":{"id":"hK1D5Z0G79il"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["string_to_ids(\"BTTTXXVVETE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wGW8TGV79lD","executionInfo":{"status":"ok","timestamp":1658410210861,"user_tz":-180,"elapsed":412,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"09204645-6cdb-4018-a21c-61bafb4fdd6f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# We can now generate the dataset, with 50% good strings, and 50% bad strings\n","# We generate the dataset based on the `embedded reber grammar`, not on the `default reber grammar` \n","def generate_dataset(size):\n","    # Generate the `good_strings` and `bad_strings`\n","    good_strings = [string_to_ids(generate_string(embedded_reber_grammar)) for _ in range(size // 2)]\n","    bad_strings  = [string_to_ids(generate_corrupted_string(embedded_reber_grammar)) for _ in range(size - size // 2)]\n","    all_strings  = good_strings + bad_strings\n","\n","    # Convert the strings to tensor (X)\n","    X = tf.ragged.constant(all_strings, ragged_rank=1)\n","\n","    # Set the labels\n","    y = np.array([[1.] for _ in range(len(good_strings))] +\n","                 [[0.] for _ in range(len(bad_strings))])\n","    \n","    # Return the training dataset with its corresponding labels\n","    return X, y"],"metadata":{"id":"xu-x1RyH7rw6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","\n","X_train, y_train = generate_dataset(10000)\n","X_valid, y_valid = generate_dataset(2000)"],"metadata":{"id":"of6X38wZ87qR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take a look at the first training sequence\n","X_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yk3vzaDy87lJ","executionInfo":{"status":"ok","timestamp":1658410382699,"user_tz":-180,"elapsed":271,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"31bff45d-7738-4ed7-9dba-06a2e7b45da3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(22,), dtype=int32, numpy=\n","array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n","      dtype=int32)>"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["y_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzXruOH7BW7e","executionInfo":{"status":"ok","timestamp":1658411382437,"user_tz":-180,"elapsed":260,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"e46234ac-0e8b-4ab9-c502-7ffd276ab7bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["X_train[0].shape, X_train[1].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7tCLrfPBV4E","executionInfo":{"status":"ok","timestamp":1658411455164,"user_tz":-180,"elapsed":337,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"b43d9f27-1843-45f4-8b11-5ad9574338b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([22]), TensorShape([11]))"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["# Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","embedding_size = 5\n","\n","model = keras.models.Sequential([\n","    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n","    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n","    keras.layers.GRU(30),\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.95, nesterov=True)\n","model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"],"metadata":{"id":"g_92lsEF85hV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1gindzjCQT0","executionInfo":{"status":"ok","timestamp":1658411622030,"user_tz":-180,"elapsed":18,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"59c6a59d-a09b-45dc-a447-df9fbe56149c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 5)           35        \n","                                                                 \n"," gru (GRU)                   (None, 30)                3330      \n","                                                                 \n"," dense (Dense)               (None, 1)                 31        \n","                                                                 \n","=================================================================\n","Total params: 3,396\n","Trainable params: 3,396\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei1hsI_885j4","executionInfo":{"status":"ok","timestamp":1658410552447,"user_tz":-180,"elapsed":97381,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"9ae67a10-1cb0-43a3-a1c7-fe828d1c513e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 5), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential/gru/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"shape. This may consume a large amount of memory.\" % value)\n"]},{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 9s 16ms/step - loss: 0.6910 - accuracy: 0.5095 - val_loss: 0.6825 - val_accuracy: 0.5645\n","Epoch 2/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6678 - accuracy: 0.5659 - val_loss: 0.6635 - val_accuracy: 0.6105\n","Epoch 3/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6504 - accuracy: 0.5766 - val_loss: 0.6521 - val_accuracy: 0.6110\n","Epoch 4/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6347 - accuracy: 0.5980 - val_loss: 0.6224 - val_accuracy: 0.6445\n","Epoch 5/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.6054 - accuracy: 0.6361 - val_loss: 0.5779 - val_accuracy: 0.6980\n","Epoch 6/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.5414 - accuracy: 0.7093 - val_loss: 0.4695 - val_accuracy: 0.7795\n","Epoch 7/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.4060 - accuracy: 0.8238 - val_loss: 0.4612 - val_accuracy: 0.7720\n","Epoch 8/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3009 - accuracy: 0.8869 - val_loss: 0.2229 - val_accuracy: 0.9255\n","Epoch 9/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1907 - accuracy: 0.9398 - val_loss: 0.1403 - val_accuracy: 0.9575\n","Epoch 10/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2069 - accuracy: 0.9317 - val_loss: 0.2536 - val_accuracy: 0.9065\n","Epoch 11/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.0940 - val_accuracy: 0.9725\n","Epoch 12/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0306 - accuracy: 0.9936 - val_loss: 0.0275 - val_accuracy: 0.9945\n","Epoch 13/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0153 - accuracy: 0.9974 - val_loss: 0.0122 - val_accuracy: 0.9985\n","Epoch 14/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0152 - accuracy: 0.9971 - val_loss: 0.0114 - val_accuracy: 0.9985\n","Epoch 15/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.0114 - val_accuracy: 0.9985\n","Epoch 16/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0085 - accuracy: 0.9987 - val_loss: 0.0110 - val_accuracy: 0.9985\n","Epoch 17/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.0120 - val_accuracy: 0.9985\n","Epoch 18/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0085 - accuracy: 0.9988 - val_loss: 0.0117 - val_accuracy: 0.9985\n","Epoch 19/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.0110 - val_accuracy: 0.9985\n","Epoch 20/20\n","313/313 [==============================] - 4s 12ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.0110 - val_accuracy: 0.9985\n"]}]},{"cell_type":"code","source":["# Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character.\n","# If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter.\n","# That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."],"metadata":{"id":"E1UCzNXm-fS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n","                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n","X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n","\n","y_proba = model.predict(X_test)\n","print(\"Estimated probability that these are Reber strings:\")\n","for index, string in enumerate(test_strings):\n","    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJVNa74b85mD","executionInfo":{"status":"ok","timestamp":1658410665082,"user_tz":-180,"elapsed":253,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"35a6e01e-e743-414a-b712-182f36b26a0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimated probability that these are Reber strings:\n","BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.01%\n","BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.33%\n"]}]},{"cell_type":"code","source":["# It worked fine. The RNN found the correct answers with very high confidence :)"],"metadata":{"id":"1R3VjGFk-Zl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"cE1dzweYDIWZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FqeZWcOxDIkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KZSMvhai-_dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 9. Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”)"],"metadata":{"id":"kcPinTeR-xJz","executionInfo":{"status":"ok","timestamp":1658475456207,"user_tz":-180,"elapsed":269,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31"],"metadata":{"id":"B-Zq79yQ-xL8","executionInfo":{"status":"ok","timestamp":1658475456872,"user_tz":-180,"elapsed":370,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from datetime import date\n","\n","# cannot use strftime()'s %B format since it depends on the locale\n","MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n","          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n","\n","def random_dates(n_dates):\n","    min_date = date(1000, 1, 1).toordinal()\n","    max_date = date(9999, 12, 31).toordinal()\n","\n","    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n","    dates    = [date.fromordinal(ordinal) for ordinal in ordinals]\n","\n","    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n","    y = [dt.isoformat() for dt in dates]\n","    \n","    return x, y"],"metadata":{"id":"9zhKCKMV_EGN","executionInfo":{"status":"ok","timestamp":1658475456873,"user_tz":-180,"elapsed":37,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Here are a few random dates, displayed in both the input format and the target format\n","np.random.seed(42)\n","\n","n_dates = 3\n","x_example, y_example = random_dates(n_dates)\n","print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n","print(\"-\" * 50)\n","for idx in range(n_dates):\n","    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_ucc6tU_EKd","executionInfo":{"status":"ok","timestamp":1658475456874,"user_tz":-180,"elapsed":37,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"5f83b3c7-eac6-44b0-d151-9d3870acd6df"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Input                    Target                   \n","--------------------------------------------------\n","September 20, 7075       7075-09-20               \n","May 15, 8579             8579-05-15               \n","January 11, 7103         7103-01-11               \n"]}]},{"cell_type":"code","source":["# Let's get the list of all possible characters in the inputs\n","INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n","INPUT_CHARS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"F-KHxiwu_ENt","executionInfo":{"status":"ok","timestamp":1658475456875,"user_tz":-180,"elapsed":36,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"d3aa7298-0653-4e28-cf46-aa3052657b31"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' ,0123456789ADFJMNOSabceghilmnoprstuvy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# And here's the list of possible characters in the outputs\n","OUTPUT_CHARS = \"0123456789-\""],"metadata":{"id":"8IvDAEuq-xOO","executionInfo":{"status":"ok","timestamp":1658475456876,"user_tz":-180,"elapsed":33,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise\n","def date_str_to_ids(date_str, chars=INPUT_CHARS):\n","    return [chars.index(c) for c in date_str]"],"metadata":{"id":"9UoAfgdQ-xQa","executionInfo":{"status":"ok","timestamp":1658475456876,"user_tz":-180,"elapsed":32,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["x_example[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"X9qyZWIaTQMl","executionInfo":{"status":"ok","timestamp":1658475456877,"user_tz":-180,"elapsed":32,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"ca59aacc-7798-467d-978f-c68bc0ce1f16"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'September 20, 7075'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["date_str_to_ids(x_example[0], INPUT_CHARS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GliV4ns_MetG","executionInfo":{"status":"ok","timestamp":1658475456877,"user_tz":-180,"elapsed":30,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"189abb7a-583e-4e74-9b78-49fef56198ef"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["y_example[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_OMTtlhQTRmg","executionInfo":{"status":"ok","timestamp":1658475456878,"user_tz":-180,"elapsed":25,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"a60ec156-5037-4aec-f6df-6d6b261615ab"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'7075-09-20'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["date_str_to_ids(y_example[0], OUTPUT_CHARS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P79l4AolMevd","executionInfo":{"status":"ok","timestamp":1658475456878,"user_tz":-180,"elapsed":23,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"f91118d5-e0ad-42ba-b4fc-057bcc19d3d1"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n","    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n","    X     = tf.ragged.constant(X_ids, ragged_rank=1)\n","    return (X + 1).to_tensor() # using 0 as the padding token ID\n","\n","def create_dataset(n_dates):\n","    x, y = random_dates(n_dates)\n","    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"],"metadata":{"id":"dbL7ITB0Mexu","executionInfo":{"status":"ok","timestamp":1658475456879,"user_tz":-180,"elapsed":19,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["np.random.seed(42)\n","\n","X_train, Y_train = create_dataset(10000)\n","X_valid, Y_valid = create_dataset(2000)\n","X_test,  Y_test  = create_dataset(2000)"],"metadata":{"id":"JTFrNkcFMe0C","executionInfo":{"status":"ok","timestamp":1658475463660,"user_tz":-180,"elapsed":6799,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["X_train[0]\n","# e.g.: September 22, 2019"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGeZzID6V-Vq","executionInfo":{"status":"ok","timestamp":1658475463661,"user_tz":-180,"elapsed":17,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"cee93358-bdf2-4e41-ee91-49f548eb7f5c"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(18,), dtype=int32, numpy=\n","array([20, 24, 32, 35, 24, 29, 22, 24, 33,  1,  5,  3,  2,  1, 10,  3, 10,\n","        8], dtype=int32)>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["Y_train[0]\n","# YYYY-MM-DD"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QitR7IJDMrjN","executionInfo":{"status":"ok","timestamp":1658475463661,"user_tz":-180,"elapsed":13,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"79e31634-9073-4cb5-eeab-bfb6f118af97"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1], dtype=int32)>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":[""],"metadata":{"id":"xC4Q5geb3ekn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## First version: a very basic seq2seq model ##\n","\n","# Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder\n","# (an embedding layer followed by a single LSTM layer), which outputs a vector,\n","# then it goes through a decoder (a single LSTM layer, followed by a dense output layer),\n","# which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n","# Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."],"metadata":{"id":"wmW37YuIMvbq","executionInfo":{"status":"ok","timestamp":1658475463662,"user_tz":-180,"elapsed":8,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["embedding_size    = 32\n","max_output_length = Y_train.shape[1]\n","\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","encoder = keras.models.Sequential([\n","    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n","                           output_dim=embedding_size,\n","                           input_shape=[None]),\n","    keras.layers.LSTM(128)\n","])\n","\n","decoder = keras.models.Sequential([\n","    keras.layers.LSTM(128, return_sequences=True),\n","    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n","])\n","\n","model = keras.models.Sequential([\n","    encoder,\n","    keras.layers.RepeatVector(max_output_length),\n","    decoder\n","])\n","\n","optimizer = keras.optimizers.Nadam()\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"],"metadata":{"id":"RHPxO59BMvd5","executionInfo":{"status":"ok","timestamp":1658475465044,"user_tz":-180,"elapsed":1390,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLeA5DwtSN7d","executionInfo":{"status":"ok","timestamp":1658475465045,"user_tz":-180,"elapsed":15,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"9f79fc3d-422b-4155-a827-17d403a7ea18"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," sequential (Sequential)     (None, 128)               83680     \n","                                                                 \n"," repeat_vector (RepeatVector  (None, 10, 128)          0         \n"," )                                                               \n","                                                                 \n"," sequential_1 (Sequential)   (None, 10, 12)            133132    \n","                                                                 \n","=================================================================\n","Total params: 216,812\n","Trainable params: 216,812\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4JmgxCRO2Yh","executionInfo":{"status":"ok","timestamp":1658475731402,"user_tz":-180,"elapsed":266363,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"569540c2-b410-49aa-adb8-61c4020103e5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","313/313 [==============================] - 18s 43ms/step - loss: 1.8161 - accuracy: 0.3484 - val_loss: 1.3736 - val_accuracy: 0.4918\n","Epoch 2/20\n","313/313 [==============================] - 12s 39ms/step - loss: 1.3035 - accuracy: 0.5314 - val_loss: 1.2254 - val_accuracy: 0.5555\n","Epoch 3/20\n","313/313 [==============================] - 12s 38ms/step - loss: 1.0354 - accuracy: 0.6250 - val_loss: 0.9800 - val_accuracy: 0.6383\n","Epoch 4/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.9410 - accuracy: 0.6598 - val_loss: 0.7848 - val_accuracy: 0.7064\n","Epoch 5/20\n","313/313 [==============================] - 12s 37ms/step - loss: 0.7745 - accuracy: 0.7174 - val_loss: 0.6251 - val_accuracy: 0.7599\n","Epoch 6/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.5330 - accuracy: 0.7915 - val_loss: 0.4507 - val_accuracy: 0.8220\n","Epoch 7/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.4427 - accuracy: 0.8374 - val_loss: 0.3493 - val_accuracy: 0.8694\n","Epoch 8/20\n","313/313 [==============================] - 12s 39ms/step - loss: 0.2742 - accuracy: 0.9017 - val_loss: 0.2297 - val_accuracy: 0.9223\n","Epoch 9/20\n","313/313 [==============================] - 12s 37ms/step - loss: 0.1803 - accuracy: 0.9471 - val_loss: 0.1396 - val_accuracy: 0.9639\n","Epoch 10/20\n","313/313 [==============================] - 12s 39ms/step - loss: 0.0976 - accuracy: 0.9779 - val_loss: 0.0939 - val_accuracy: 0.9786\n","Epoch 11/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0544 - accuracy: 0.9911 - val_loss: 0.0399 - val_accuracy: 0.9948\n","Epoch 12/20\n","313/313 [==============================] - 13s 40ms/step - loss: 0.0297 - accuracy: 0.9973 - val_loss: 0.0252 - val_accuracy: 0.9979\n","Epoch 13/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0177 - accuracy: 0.9989 - val_loss: 0.0158 - val_accuracy: 0.9992\n","Epoch 14/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0113 - accuracy: 0.9996 - val_loss: 0.0100 - val_accuracy: 0.9998\n","Epoch 15/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0074 - accuracy: 0.9999 - val_loss: 0.0071 - val_accuracy: 0.9999\n","Epoch 16/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9999\n","Epoch 17/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n","Epoch 18/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n","Epoch 19/20\n","313/313 [==============================] - 12s 38ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n","Epoch 20/20\n","313/313 [==============================] - 12s 37ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:\n","def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n","    return [\"\".join([(\"?\" + chars)[index] for index in sequence]) for sequence in ids]"],"metadata":{"id":"Lh9L-d2TMvgN","executionInfo":{"status":"ok","timestamp":1658475731403,"user_tz":-180,"elapsed":15,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"],"metadata":{"id":"qTVgwph6Mvit","executionInfo":{"status":"ok","timestamp":1658475731404,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["ids = np.argmax(model.predict(X_new), axis=-1)\n","for date_str in ids_to_date_strs(ids):\n","    print(date_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_WgWUjmMrmb","executionInfo":{"status":"ok","timestamp":1658475732280,"user_tz":-180,"elapsed":882,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"b5249f83-9644-4ba9-e3b2-d4a3db11ce6f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["2009-09-17\n","1789-07-14\n"]}]},{"cell_type":"code","source":["# Perfect! :)\n","# However, since the model was only trained on input strings of length 18 (which is the length of the longest date),\n","# it does not perform well if we try to use it to make predictions on shorter sequences:"],"metadata":{"id":"DlOHbYjSMrqx","executionInfo":{"status":"ok","timestamp":1658475732281,"user_tz":-180,"elapsed":8,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"],"metadata":{"id":"gAi4l2ZWMe2M","executionInfo":{"status":"ok","timestamp":1658475732282,"user_tz":-180,"elapsed":6,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["ids = np.argmax(model.predict(X_new), axis=-1)\n","for date_str in ids_to_date_strs(ids):\n","    print(date_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwtksmvj85oN","executionInfo":{"status":"ok","timestamp":1658475733173,"user_tz":-180,"elapsed":897,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"427f168d-82dd-4f82-8726-552a1bf561dc"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["2020-02-02\n","1789-09-14\n"]}]},{"cell_type":"code","source":["# Oops! We need to ensure that we always pass sequences of the same length as during training,\n","# using padding if necessary. Let's write a little helper function for that"],"metadata":{"id":"oj2oD4xgR8VF","executionInfo":{"status":"ok","timestamp":1658475733173,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["max_input_length = X_train.shape[1]\n","\n","def prepare_date_strs_padded(date_strs):\n","    X = prepare_date_strs(date_strs)\n","    if X.shape[1] < max_input_length:\n","        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n","    return X\n","\n","def convert_date_strs(date_strs):\n","    X = prepare_date_strs_padded(date_strs)\n","    #ids = model.predict_classes(X)\n","    ids = np.argmax(model.predict(X), axis=-1)\n","    return ids_to_date_strs(ids)"],"metadata":{"id":"MoogIuMeR8XS","executionInfo":{"status":"ok","timestamp":1658475733174,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSvsXbwESbn9","executionInfo":{"status":"ok","timestamp":1658475733174,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"01f01f8f-7bfd-4abb-af34-c76e962c7eec"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2020-05-02', '1789-07-14']"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# Cool! Granted, there are certainly much easier ways to write a date conversion tool\n","# (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler.\n","# However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."],"metadata":{"id":"D7vChCuHSbqh","executionInfo":{"status":"ok","timestamp":1658475733175,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"A8QrGpQvSbs4","executionInfo":{"status":"ok","timestamp":1658475733175,"user_tz":-180,"elapsed":7,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"1u8AldFAR8ZH","executionInfo":{"status":"ok","timestamp":1658475733175,"user_tz":-180,"elapsed":6,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["## Second version: feeding the shifted targets to the decoder (teacher forcing) ##\n","\n","# Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right.\n","# This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n","\n","# Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n","\n","# During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token,\n","# then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n","\n","# But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder?\n","# Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM\n","# (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n","\n","# Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."],"metadata":{"id":"GCl3CfhDR8a-","executionInfo":{"status":"ok","timestamp":1658477559985,"user_tz":-180,"elapsed":249,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["sos_id = len(OUTPUT_CHARS) + 1\n","\n","def shifted_output_sequences(Y):\n","    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n","    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n","\n","X_train_decoder = shifted_output_sequences(Y_train)\n","X_valid_decoder = shifted_output_sequences(Y_valid)\n","X_test_decoder  = shifted_output_sequences(Y_test)"],"metadata":{"id":"F-MspUzxWtbJ","executionInfo":{"status":"ok","timestamp":1658477606511,"user_tz":-180,"elapsed":230,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["X_train_decoder"],"metadata":{"id":"iQFiJGgcWte6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658477611296,"user_tz":-180,"elapsed":235,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"52c0ef32-1e3d-430e-83a2-8cf6ebab3faa"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n","array([[12,  8,  1, ..., 10, 11,  3],\n","       [12,  9,  6, ...,  6, 11,  2],\n","       [12,  8,  2, ...,  2, 11,  2],\n","       ...,\n","       [12, 10,  8, ...,  2, 11,  4],\n","       [12,  2,  2, ...,  3, 11,  3],\n","       [12,  8,  9, ...,  8, 11,  3]], dtype=int32)>"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API"],"metadata":{"id":"H08FSqlKWthn","executionInfo":{"status":"ok","timestamp":1658477630093,"user_tz":-180,"elapsed":235,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["encoder_embedding_size = 32\n","decoder_embedding_size = 32\n","lstm_units = 128\n","\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# Encoder - Input\n","encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n","\n","# Embedding\n","encoder_embedding = keras.layers.Embedding(\n","    input_dim=len(INPUT_CHARS) + 1,\n","    output_dim=encoder_embedding_size)(encoder_input)\n","\n","# LSTM\n","_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n","    lstm_units, return_state=True)(encoder_embedding)\n","encoder_state = [encoder_state_h, encoder_state_c]\n","\n","\n","# Decoder - Input\n","decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n","\n","# Embedding\n","decoder_embedding = keras.layers.Embedding(\n","    input_dim=len(OUTPUT_CHARS) + 2,\n","    output_dim=decoder_embedding_size)(decoder_input)\n","\n","# LSTM\n","decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n","\n","# Decoder - Output\n","decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")(decoder_lstm_output)\n","\n","\n","# Model\n","model = keras.models.Model(inputs=[encoder_input, decoder_input], outputs=[decoder_output])\n","\n","# Optimizer + Compile\n","optimizer = keras.optimizers.Nadam()\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"],"metadata":{"id":"_y2cx2fV90ax","executionInfo":{"status":"ok","timestamp":1658477853896,"user_tz":-180,"elapsed":1389,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["history = model.fit([X_train, X_train_decoder], Y_train, epochs=10, validation_data=([X_valid, X_valid_decoder], Y_valid))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_UwUF_l-XnF","executionInfo":{"status":"ok","timestamp":1658478009245,"user_tz":-180,"elapsed":149145,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"f750c55c-660f-49ea-ee46-0c485a185ac6"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","313/313 [==============================] - 20s 40ms/step - loss: 1.6826 - accuracy: 0.3733 - val_loss: 1.4075 - val_accuracy: 0.4686\n","Epoch 2/10\n","313/313 [==============================] - 12s 38ms/step - loss: 1.1944 - accuracy: 0.5541 - val_loss: 0.9220 - val_accuracy: 0.6539\n","Epoch 3/10\n","313/313 [==============================] - 11s 35ms/step - loss: 0.6502 - accuracy: 0.7660 - val_loss: 0.3609 - val_accuracy: 0.8924\n","Epoch 4/10\n","313/313 [==============================] - 14s 46ms/step - loss: 0.2216 - accuracy: 0.9451 - val_loss: 0.1234 - val_accuracy: 0.9776\n","Epoch 5/10\n","313/313 [==============================] - 17s 55ms/step - loss: 0.0695 - accuracy: 0.9928 - val_loss: 0.0417 - val_accuracy: 0.9992\n","Epoch 6/10\n","313/313 [==============================] - 17s 55ms/step - loss: 0.0570 - accuracy: 0.9925 - val_loss: 0.0225 - val_accuracy: 0.9998\n","Epoch 7/10\n","313/313 [==============================] - 12s 37ms/step - loss: 0.0162 - accuracy: 0.9998 - val_loss: 0.0138 - val_accuracy: 0.9998\n","Epoch 8/10\n","313/313 [==============================] - 11s 35ms/step - loss: 0.0455 - accuracy: 0.9912 - val_loss: 0.0115 - val_accuracy: 0.9999\n","Epoch 9/10\n","313/313 [==============================] - 11s 36ms/step - loss: 0.0081 - accuracy: 0.9999 - val_loss: 0.0069 - val_accuracy: 0.9999\n","Epoch 10/10\n","313/313 [==============================] - 12s 40ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9999\n"]}]},{"cell_type":"code","source":["# Let's once again use the model to make some predictions. This time we need to predict characters one by one"],"metadata":{"id":"MvaVV3Jv-XpP","executionInfo":{"status":"ok","timestamp":1658478247690,"user_tz":-180,"elapsed":261,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["sos_id = len(OUTPUT_CHARS) + 1\n","\n","def predict_date_strs(date_strs):\n","    X = prepare_date_strs_padded(date_strs)\n","    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n","    \n","    for index in range(max_output_length):\n","        pad_size      = max_output_length - Y_pred.shape[1]\n","        X_decoder     = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n","        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n","        Y_pred_next   = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n","        Y_pred        = tf.concat([Y_pred, Y_pred_next], axis=1)\n","\n","    return ids_to_date_strs(Y_pred[:, 1:])"],"metadata":{"id":"NuSxkFkV-XrX","executionInfo":{"status":"ok","timestamp":1658478283265,"user_tz":-180,"elapsed":233,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7CFfcv4AcqZ","executionInfo":{"status":"ok","timestamp":1658478294569,"user_tz":-180,"elapsed":2431,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}},"outputId":"fa4ce0d2-0bf6-4016-ad52-67fa5d528d19"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1789-07-14', '2020-05-01']"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# Works fine! :)"],"metadata":{"id":"rm06b5ntAcsj","executionInfo":{"status":"ok","timestamp":1658478302271,"user_tz":-180,"elapsed":3,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"W0zziVVOAcuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fXG8UnFmApbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Third version: using TF-Addons's seq2seq implementation ##"],"metadata":{"id":"YtdLns6LApda","executionInfo":{"status":"ok","timestamp":1658478321332,"user_tz":-180,"elapsed":3,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# import tensorflow_addons as tfa\n","\n","# np.random.seed(42)\n","# tf.random.set_seed(42)\n","\n","# encoder_embedding_size = 32\n","# decoder_embedding_size = 32\n","# units = 128\n","\n","# encoder_inputs   = keras.layers.Input(shape=[None], dtype=np.int32)\n","# decoder_inputs   = keras.layers.Input(shape=[None], dtype=np.int32)\n","# sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n","\n","# encoder_embeddings = keras.layers.Embedding(len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n","\n","# decoder_embedding_layer = keras.layers.Embedding(len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n","# decoder_embeddings      = decoder_embedding_layer(decoder_inputs)\n","\n","# encoder = keras.layers.LSTM(units, return_state=True)\n","# encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n","# encoder_state = [state_h, state_c]\n","\n","# sampler = tfa.seq2seq.sampler.TrainingSampler()\n","\n","# decoder_cell = keras.layers.LSTMCell(units)\n","# output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n","\n","# decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n","#                                                  sampler,\n","#                                                  output_layer=output_layer)\n","\n","# final_outputs, final_state, final_sequence_lengths = decoder(\n","#     decoder_embeddings,\n","#     initial_state=encoder_state)\n","# Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n","\n","# # Model\n","# model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n","# optimizer = keras.optimizers.Nadam()\n","# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n","# history = model.fit([X_train, X_train_decoder], Y_train, epochs=15, validation_data=([X_valid, X_valid_decoder], Y_valid))"],"metadata":{"id":"XIk66iaQApfM","executionInfo":{"status":"ok","timestamp":1658478407684,"user_tz":-180,"elapsed":246,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["## Fourth version: using TF-Addons's seq2seq implementation with a scheduled sampler ##"],"metadata":{"id":"M_6_QSCsAphI","executionInfo":{"status":"ok","timestamp":1658478501661,"user_tz":-180,"elapsed":242,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["## Fifth version: using TFA seq2seq, the Keras subclassing API and attention mechanisms ##"],"metadata":{"id":"96O_FvGNBawP","executionInfo":{"status":"ok","timestamp":1658478515276,"user_tz":-180,"elapsed":248,"user":{"displayName":"Iulian Taiatu","userId":"08231486411260347125"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2ISsmfrSBdc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FYn-CdY790ez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ox7n6UlV90hJ"},"execution_count":null,"outputs":[]}]}